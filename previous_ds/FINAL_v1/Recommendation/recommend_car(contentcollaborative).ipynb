{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_ID</th>\n",
       "      <th>travelCode</th>\n",
       "      <th>Check-in</th>\n",
       "      <th>pickupLocation</th>\n",
       "      <th>dropoffLocation</th>\n",
       "      <th>carType</th>\n",
       "      <th>rentalAgency</th>\n",
       "      <th>rentalDuration</th>\n",
       "      <th>Car_total_distance</th>\n",
       "      <th>fuelPolicy</th>\n",
       "      <th>Car_bookingStatus</th>\n",
       "      <th>total_rent_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>02/27/2020</td>\n",
       "      <td>Johntown</td>\n",
       "      <td>Port Brian</td>\n",
       "      <td>Sedan</td>\n",
       "      <td>Sixt</td>\n",
       "      <td>3</td>\n",
       "      <td>285</td>\n",
       "      <td>Prepaid</td>\n",
       "      <td>Pending</td>\n",
       "      <td>3410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>08/06/2020</td>\n",
       "      <td>Owensland</td>\n",
       "      <td>Ruizfort</td>\n",
       "      <td>Sedan</td>\n",
       "      <td>Enterprise</td>\n",
       "      <td>3</td>\n",
       "      <td>412</td>\n",
       "      <td>Full-to-Full</td>\n",
       "      <td>Pending</td>\n",
       "      <td>4644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>117</td>\n",
       "      <td>02/27/2020</td>\n",
       "      <td>Edwardview</td>\n",
       "      <td>Katiefort</td>\n",
       "      <td>Sedan</td>\n",
       "      <td>Hertz</td>\n",
       "      <td>1</td>\n",
       "      <td>433</td>\n",
       "      <td>Partial</td>\n",
       "      <td>Confirmed</td>\n",
       "      <td>4740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>119</td>\n",
       "      <td>03/12/2020</td>\n",
       "      <td>Moranborough</td>\n",
       "      <td>Lake Stephanie</td>\n",
       "      <td>Hatchback</td>\n",
       "      <td>Budget</td>\n",
       "      <td>3</td>\n",
       "      <td>421</td>\n",
       "      <td>Full-to-Full</td>\n",
       "      <td>Confirmed</td>\n",
       "      <td>3744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>153</td>\n",
       "      <td>11/05/2020</td>\n",
       "      <td>Port Kathrynstad</td>\n",
       "      <td>East Ronnieberg</td>\n",
       "      <td>Sedan</td>\n",
       "      <td>Enterprise</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>Prepaid</td>\n",
       "      <td>Cancelled</td>\n",
       "      <td>1694</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User_ID  travelCode    Check-in    pickupLocation  dropoffLocation  \\\n",
       "0        0          22  02/27/2020          Johntown       Port Brian   \n",
       "1        0          45  08/06/2020         Owensland         Ruizfort   \n",
       "2        2         117  02/27/2020        Edwardview        Katiefort   \n",
       "3        2         119  03/12/2020      Moranborough   Lake Stephanie   \n",
       "4        2         153  11/05/2020  Port Kathrynstad  East Ronnieberg   \n",
       "\n",
       "     carType rentalAgency  rentalDuration  Car_total_distance    fuelPolicy  \\\n",
       "0      Sedan         Sixt               3                 285       Prepaid   \n",
       "1      Sedan   Enterprise               3                 412  Full-to-Full   \n",
       "2      Sedan        Hertz               1                 433       Partial   \n",
       "3  Hatchback       Budget               3                 421  Full-to-Full   \n",
       "4      Sedan   Enterprise               5                 100       Prepaid   \n",
       "\n",
       "  Car_bookingStatus  total_rent_price  \n",
       "0           Pending              3410  \n",
       "1           Pending              4644  \n",
       "2         Confirmed              4740  \n",
       "3         Confirmed              3744  \n",
       "4         Cancelled              1694  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "car_df=pd.read_csv(\"D:\\\\Make_my_trip\\\\Recommendation\\\\cars.csv\")\n",
    "car_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_car=car_df.head().to_csv(\"D:\\\\Make_my_trip\\\\Recommendation\\\\sample_car.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the entire dataset of flight, hotel and cars booking.\n",
    "whole_ds=pd.read_csv(\"D:\\\\Make_my_trip\\\\2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>travelCode</th>\n",
       "      <th>user_id</th>\n",
       "      <th>departure</th>\n",
       "      <th>arrival</th>\n",
       "      <th>flightType</th>\n",
       "      <th>flight_price</th>\n",
       "      <th>flight_duration</th>\n",
       "      <th>distance</th>\n",
       "      <th>agency</th>\n",
       "      <th>flight_date</th>\n",
       "      <th>...</th>\n",
       "      <th>pickupLocation</th>\n",
       "      <th>dropoffLocation</th>\n",
       "      <th>carType</th>\n",
       "      <th>rentalAgency</th>\n",
       "      <th>rentalDuration</th>\n",
       "      <th>totalDistance</th>\n",
       "      <th>fuelPolicy</th>\n",
       "      <th>bookingStatus</th>\n",
       "      <th>total_rent_price</th>\n",
       "      <th>total_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Recife (PE)</td>\n",
       "      <td>Florianopolis (SC)</td>\n",
       "      <td>firstClass</td>\n",
       "      <td>1434.38</td>\n",
       "      <td>1.76</td>\n",
       "      <td>676.53</td>\n",
       "      <td>FlyingDrops</td>\n",
       "      <td>09/26/2019</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2686.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Aracaju (SE)</td>\n",
       "      <td>Salvador (BH)</td>\n",
       "      <td>firstClass</td>\n",
       "      <td>1684.05</td>\n",
       "      <td>2.16</td>\n",
       "      <td>830.86</td>\n",
       "      <td>CloudFy</td>\n",
       "      <td>10/10/2019</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2210.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>Aracaju (SE)</td>\n",
       "      <td>Salvador (BH)</td>\n",
       "      <td>economic</td>\n",
       "      <td>964.83</td>\n",
       "      <td>2.16</td>\n",
       "      <td>830.86</td>\n",
       "      <td>CloudFy</td>\n",
       "      <td>11/14/2019</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1755.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>Brasilia (DF)</td>\n",
       "      <td>Salvador (BH)</td>\n",
       "      <td>premium</td>\n",
       "      <td>1268.97</td>\n",
       "      <td>1.76</td>\n",
       "      <td>676.56</td>\n",
       "      <td>Rainbow</td>\n",
       "      <td>12/12/2019</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2322.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>Recife (PE)</td>\n",
       "      <td>Florianopolis (SC)</td>\n",
       "      <td>firstClass</td>\n",
       "      <td>1434.38</td>\n",
       "      <td>1.76</td>\n",
       "      <td>676.53</td>\n",
       "      <td>FlyingDrops</td>\n",
       "      <td>12/26/2019</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1747.40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   travelCode  user_id      departure             arrival  flightType  \\\n",
       "0           0        0    Recife (PE)  Florianopolis (SC)  firstClass   \n",
       "1           2        0   Aracaju (SE)       Salvador (BH)  firstClass   \n",
       "2           7        0   Aracaju (SE)       Salvador (BH)    economic   \n",
       "3          11        0  Brasilia (DF)       Salvador (BH)     premium   \n",
       "4          13        0    Recife (PE)  Florianopolis (SC)  firstClass   \n",
       "\n",
       "   flight_price  flight_duration  distance       agency flight_date  ...  \\\n",
       "0       1434.38             1.76    676.53  FlyingDrops  09/26/2019  ...   \n",
       "1       1684.05             2.16    830.86      CloudFy  10/10/2019  ...   \n",
       "2        964.83             2.16    830.86      CloudFy  11/14/2019  ...   \n",
       "3       1268.97             1.76    676.56      Rainbow  12/12/2019  ...   \n",
       "4       1434.38             1.76    676.53  FlyingDrops  12/26/2019  ...   \n",
       "\n",
       "  pickupLocation dropoffLocation  carType  rentalAgency  rentalDuration  \\\n",
       "0            NaN             NaN      NaN           NaN             NaN   \n",
       "1            NaN             NaN      NaN           NaN             NaN   \n",
       "2            NaN             NaN      NaN           NaN             NaN   \n",
       "3            NaN             NaN      NaN           NaN             NaN   \n",
       "4            NaN             NaN      NaN           NaN             NaN   \n",
       "\n",
       "  totalDistance fuelPolicy bookingStatus total_rent_price  total_price  \n",
       "0           NaN        NaN           NaN              NaN      2686.46  \n",
       "1           NaN        NaN           NaN              NaN      2210.87  \n",
       "2           NaN        NaN           NaN              NaN      1755.06  \n",
       "3           NaN        NaN           NaN              NaN      2322.61  \n",
       "4           NaN        NaN           NaN              NaN      1747.40  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_whole=whole_ds.head().to_csv(\"D:\\\\Make_my_trip\\\\Recommendation\\\\sample_ds.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['travelCode', 'user_id', 'departure', 'arrival', 'flightType',\n",
       "       'flight_price', 'flight_duration', 'distance', 'agency', 'flight_date',\n",
       "       'hotel_name', 'hotel_place', 'days', 'hotel_base', 'total_hotel',\n",
       "       'check_in', 'company', 'user_name', 'gender', 'age', 'pickupLocation',\n",
       "       'dropoffLocation', 'carType', 'rentalAgency', 'rentalDuration',\n",
       "       'totalDistance', 'fuelPolicy', 'bookingStatus', 'total_rent_price',\n",
       "       'total_price'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_ds.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "following are the fields needed for recommendation:\n",
    "1. user_id: Unique identifier for each user\n",
    "2. booking_id: Unique booking identifier\n",
    "3. destination: Trip destination\n",
    "4. trip_duration: Number of days for the trip\n",
    "5. num_travelers: Number of people traveling\n",
    "6. flight_date: Date of flight booking\n",
    "7. hotel_booked: Boolean indicating hotel booking\n",
    "8. hotel_destination: Hotel destination\n",
    "9. car_booked: Boolean indicating car booking\n",
    "10. user_preferences: Optional field for additional user preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "required_fields=['user_id','travelCode',]# these are the fields that are to be kept from the whole_ds dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Car Recommendation\n",
    "used Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 1.0\n",
      "\n",
      "Car Rental Recommendations:\n",
      "\n",
      "Booking ID: 0\n",
      "Destination: Florianopolis (SC)\n",
      "Recommended Car Type: Luxury\n",
      "Pickup Location: Florianopolis (SC)\n",
      "Dropoff Location: Florianopolis (SC)\n",
      "Suggested Duration: 4 days\n",
      "Estimated Price: $626.04\n",
      "Reason: Long distance travel, Extended stay duration, Premium travel preference (Model Recommendation)\n",
      "\n",
      "Booking ID: 2\n",
      "Destination: Salvador (BH)\n",
      "Recommended Car Type: Luxury\n",
      "Pickup Location: Salvador (BH)\n",
      "Dropoff Location: Salvador (BH)\n",
      "Suggested Duration: 2 days\n",
      "Estimated Price: $263.41\n",
      "Reason: Long distance travel, Premium travel preference (Model Recommendation)\n",
      "\n",
      "Booking ID: 7\n",
      "Destination: Salvador (BH)\n",
      "Recommended Car Type: Economy\n",
      "Pickup Location: Salvador (BH)\n",
      "Dropoff Location: Salvador (BH)\n",
      "Suggested Duration: 3 days\n",
      "Estimated Price: $395.12\n",
      "Reason: Long distance travel, Extended stay duration (Model Recommendation)\n",
      "\n",
      "Booking ID: 11\n",
      "Destination: Salvador (BH)\n",
      "Recommended Car Type: Luxury\n",
      "Pickup Location: Salvador (BH)\n",
      "Dropoff Location: Salvador (BH)\n",
      "Suggested Duration: 4 days\n",
      "Estimated Price: $526.82\n",
      "Reason: Long distance travel, Extended stay duration, Premium travel preference (Model Recommendation)\n",
      "\n",
      "Booking ID: 13\n",
      "Destination: Florianopolis (SC)\n",
      "Recommended Car Type: Luxury\n",
      "Pickup Location: Florianopolis (SC)\n",
      "Dropoff Location: Florianopolis (SC)\n",
      "Suggested Duration: 1 days\n",
      "Estimated Price: $156.51\n",
      "Reason: Long distance travel, Premium travel preference (Model Recommendation)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier  # Example model\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class TravelDataProcessor:\n",
    "    def __init__(self, data_path):\n",
    "        self.raw_data = pd.read_csv(data_path)\n",
    "        self.processed_data = None\n",
    "        self.model = None  # Initialize model here\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        df = self.raw_data.copy()\n",
    "\n",
    "        # Convert dates to datetime\n",
    "        df['flight_date'] = pd.to_datetime(df['flight_date'])\n",
    "        df['check_in'] = pd.to_datetime(df['check_in'])\n",
    "\n",
    "        # Calculate check_out date\n",
    "        df['check_out'] = df['check_in'] + pd.to_timedelta(df['days'], unit='D')\n",
    "\n",
    "        # Engineer features relevant for car recommendations\n",
    "        df['trip_duration'] = df['days']\n",
    "        df['is_long_distance'] = df['distance'] > df['distance'].median()\n",
    "\n",
    "        # Calculate potential car need features\n",
    "        df['needs_airport_transfer'] = True  # Since all bookings include flights\n",
    "\n",
    "        # Calculate price per day for reference\n",
    "        df['price_per_day'] = df['total_price'] / df['days']\n",
    "\n",
    "        # Create binary features\n",
    "        df['is_business_class'] = df['flightType'].isin(['firstClass', 'premium'])\n",
    "        df['is_expensive_hotel'] = df['hotel_base'] > df['hotel_base'].median()\n",
    "\n",
    "        # Create potential car rental locations (can be improved)\n",
    "        df['suggested_pickup_location'] = df['arrival']\n",
    "        df['suggested_dropoff_location'] = df['arrival']\n",
    "\n",
    "        # Suggest car rental duration (same as hotel stay)\n",
    "        df['suggested_rental_duration'] = df['days']\n",
    "\n",
    "        # Create feature for potential car recommendation (used for training initially)\n",
    "        df['recommend_car'] = (\n",
    "            (df['distance'] > 500) |\n",
    "            (df['days'] >= 3) |\n",
    "            (df['price_per_day'] > df['price_per_day'].median())\n",
    "        )\n",
    "\n",
    "        self.processed_data = df\n",
    "        return df\n",
    "\n",
    "    def train_car_recommendation_model(self):\n",
    "        df = self.processed_data.copy()\n",
    "\n",
    "        features = [\n",
    "            'trip_duration',\n",
    "            'is_long_distance',\n",
    "            'is_business_class',\n",
    "            'is_expensive_hotel',\n",
    "            'price_per_day'\n",
    "        ]\n",
    "\n",
    "        X = df[features]\n",
    "        y = df['recommend_car'].astype(int)\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        self.model = RandomForestClassifier(random_state=42)\n",
    "        self.model.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = self.model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        print(f\"Model Accuracy: {accuracy}\")\n",
    "\n",
    "    def generate_car_recommendations(self, user_id):\n",
    "        user_data = self.processed_data[self.processed_data['user_id'] == user_id].copy()\n",
    "\n",
    "        recommendations = []\n",
    "\n",
    "        for _, booking in user_data.iterrows():\n",
    "            features = [\n",
    "                booking['trip_duration'],\n",
    "                booking['is_long_distance'],\n",
    "                booking['is_business_class'],\n",
    "                booking['is_expensive_hotel'],\n",
    "                booking['price_per_day']\n",
    "            ]\n",
    "            X_user = pd.DataFrame([features], columns=[\n",
    "                'trip_duration', 'is_long_distance', 'is_business_class', 'is_expensive_hotel', 'price_per_day'\n",
    "            ])\n",
    "\n",
    "            if self.model is not None:  # Check if the model is trained\n",
    "                should_recommend = self.model.predict(X_user)[0]\n",
    "            else:\n",
    "                should_recommend = False  # Or use a default rule if not trained\n",
    "\n",
    "            if should_recommend:\n",
    "                rec = {\n",
    "                    'booking_id': booking['travelCode'],\n",
    "                    'destination': booking['arrival'],\n",
    "                    'recommended_car_type': self._suggest_car_type(booking),\n",
    "                    'pickup_location': booking['arrival'],\n",
    "                    'dropoff_location': booking['arrival'],\n",
    "                    'suggested_duration': booking['days'],\n",
    "                    'estimated_price': self._estimate_car_price(booking),\n",
    "                    'recommendation_reason': self._get_recommendation_reason(booking) + \" (Model Recommendation)\"\n",
    "                }\n",
    "                recommendations.append(rec)\n",
    "\n",
    "        return recommendations\n",
    "\n",
    "    def _suggest_car_type(self, booking):\n",
    "        # ... (Existing logic remains the same)\n",
    "        if booking['flightType'] in ['firstClass', 'premium']:\n",
    "            return 'Luxury'\n",
    "        elif booking['total_price'] > self.processed_data['total_price'].median():\n",
    "            return 'Premium'\n",
    "        elif booking['days'] > 3:\n",
    "            return 'Midsize'\n",
    "        else:\n",
    "            return 'Economy'\n",
    "\n",
    "    def _estimate_car_price(self, booking):\n",
    "        # ... (Existing logic remains the same)\n",
    "        base_price_per_day = booking['hotel_base'] * 0.5\n",
    "        return base_price_per_day * booking['days']\n",
    "\n",
    "    def _get_recommendation_reason(self, booking):\n",
    "        # ... (Existing logic remains the same)\n",
    "        reasons = []\n",
    "\n",
    "        if booking['distance'] > 500:\n",
    "            reasons.append(\"Long distance travel\")\n",
    "        if booking['days'] >= 3:\n",
    "            reasons.append(\"Extended stay duration\")\n",
    "        if booking['flightType'] in ['firstClass', 'premium']:\n",
    "            reasons.append(\"Premium travel preference\")\n",
    "\n",
    "        return \", \".join(reasons) if reasons else \"Convenience for local travel\"\n",
    "\n",
    "\n",
    "def main():\n",
    "    processor = TravelDataProcessor('D:\\\\Make_my_trip\\\\Recommendation\\\\sample_ds.csv')  # Replace with your data path\n",
    "    processed_data = processor.preprocess_data()\n",
    "\n",
    "    processor.train_car_recommendation_model()  # Train the model\n",
    "\n",
    "    recommendations = processor.generate_car_recommendations(0)  # User ID 0\n",
    "\n",
    "    print(\"\\nCar Rental Recommendations:\")\n",
    "    for rec in recommendations:\n",
    "        print(\"\\nBooking ID:\", rec['booking_id'])\n",
    "        print(f\"Destination: {rec['destination']}\")\n",
    "        print(f\"Recommended Car Type: {rec['recommended_car_type']}\")\n",
    "        print(f\"Pickup Location: {rec['pickup_location']}\")\n",
    "        print(f\"Dropoff Location: {rec['dropoff_location']}\")\n",
    "        print(f\"Suggested Duration: {rec['suggested_duration']} days\")\n",
    "        print(f\"Estimated Price: ${rec['estimated_price']:.2f}\")\n",
    "        print(f\"Reason: {rec['recommendation_reason']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars=pd.read_excel(r\"D:\\Make_my_trip\\FinalDataset\\CarFINALdataset.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars.to_csv(\"carsfds.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Comparing different ML models and evaluating each of the models performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recommend_car\n",
      "1    16099\n",
      "Name: count, dtype: int64\n",
      "Training XGBoost...\n",
      "XGBoost Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.51      0.49      1537\n",
      "           1       0.52      0.49      0.51      1683\n",
      "\n",
      "    accuracy                           0.50      3220\n",
      "   macro avg       0.50      0.50      0.50      3220\n",
      "weighted avg       0.50      0.50      0.50      3220\n",
      "\n",
      "Accuracy: 0.5019\n",
      "\n",
      "Training Random Forest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Make_my_trip\\Recommendation\\recomm\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [14:56:55] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.53      0.51      1537\n",
      "           1       0.53      0.48      0.50      1683\n",
      "\n",
      "    accuracy                           0.50      3220\n",
      "   macro avg       0.50      0.50      0.50      3220\n",
      "weighted avg       0.51      0.50      0.50      3220\n",
      "\n",
      "Accuracy: 0.5028\n",
      "\n",
      "Training Decision Tree...\n",
      "Decision Tree Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.51      0.49      1537\n",
      "           1       0.52      0.49      0.51      1683\n",
      "\n",
      "    accuracy                           0.50      3220\n",
      "   macro avg       0.50      0.50      0.50      3220\n",
      "weighted avg       0.50      0.50      0.50      3220\n",
      "\n",
      "Accuracy: 0.5019\n",
      "\n",
      "Training Logistic Regression...\n",
      "Logistic Regression Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.77      0.59      1537\n",
      "           1       0.52      0.23      0.32      1683\n",
      "\n",
      "    accuracy                           0.49      3220\n",
      "   macro avg       0.50      0.50      0.46      3220\n",
      "weighted avg       0.50      0.49      0.45      3220\n",
      "\n",
      "Accuracy: 0.4885\n",
      "\n",
      "Training SVM...\n",
      "SVM Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.37      0.42      1537\n",
      "           1       0.53      0.64      0.58      1683\n",
      "\n",
      "    accuracy                           0.51      3220\n",
      "   macro avg       0.51      0.51      0.50      3220\n",
      "weighted avg       0.51      0.51      0.50      3220\n",
      "\n",
      "Accuracy: 0.5127\n",
      "\n",
      "Best Model: SVM with Accuracy: 0.5127\n"
     ]
    }
   ],
   "source": [
    "###  comparing different ML models:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "class RecommendationProcessor:\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self.processed_data = None\n",
    "        self.models = {}\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        df = pd.read_csv(self.file_path)\n",
    "\n",
    "        # Identify date columns and parse them\n",
    "        date_columns = [col for col in ['flight_date', 'check_in', 'check_out'] if col in df.columns]\n",
    "        for col in date_columns:\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce', format='%d-%m-%Y')\n",
    "\n",
    "        # Drop rows where any date is missing\n",
    "        if date_columns:\n",
    "            df.dropna(subset=date_columns, inplace=True)\n",
    "\n",
    "        # Feature Engineering\n",
    "        df['days'] = (df['check_out'] - df['check_in']).dt.days if 'check_out' in df.columns else 1\n",
    "        df['distance'] = df['distance'].fillna(df['distance'].median()) if 'distance' in df.columns else 100\n",
    "        df['price_per_day'] = df['price_per_day'].fillna(df['price_per_day'].median()) if 'price_per_day' in df.columns else 500\n",
    "\n",
    "        # Create target variable with at least two classes\n",
    "        df['recommend_car'] = (\n",
    "            (df['distance'] > df['distance'].median() * 1.1) |  \n",
    "            (df['days'] >= df['days'].median() * 0.9) |  \n",
    "            (df['price_per_day'] > df['price_per_day'].median() * 1.1)\n",
    "        ).astype(int)\n",
    "\n",
    "        print(df['recommend_car'].value_counts())  # Debugging step to verify class balance\n",
    "        if df['recommend_car'].nunique() == 1:\n",
    "            df['recommend_car'] = np.random.choice([0, 1], size=len(df), p=[0.5, 0.5])\n",
    "\n",
    "        self.processed_data = df\n",
    "\n",
    "    def train_models(self):\n",
    "        df = self.processed_data\n",
    "        feature_columns = ['distance', 'days', 'price_per_day']\n",
    "        X = df[[col for col in feature_columns if col in df.columns]]\n",
    "        y = df['recommend_car']\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        models = {\n",
    "            'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
    "            'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "            'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "            'Logistic Regression': LogisticRegression(),\n",
    "            'SVM': SVC()\n",
    "        }\n",
    "\n",
    "        best_model = None\n",
    "        best_accuracy = 0\n",
    "\n",
    "        for name, model in models.items():\n",
    "            print(f\"Training {name}...\")\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            print(f\"{name} Performance:\")\n",
    "            print(classification_report(y_test, y_pred))\n",
    "            print(f\"Accuracy: {accuracy:.4f}\\n\")\n",
    "\n",
    "            self.models[name] = model\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_model = name\n",
    "\n",
    "        print(f\"Best Model: {best_model} with Accuracy: {best_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    processor = RecommendationProcessor(\"D:\\\\Make_my_trip\\\\previous_ds\\\\2.csv\")\n",
    "    processor.preprocess_data()\n",
    "    processor.train_models()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refining the performance of each ML model(pipeline):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hetvi\\AppData\\Local\\Temp\\ipykernel_1808\\3160890175.py:23: UserWarning: Parsing dates in %m/%d/%Y format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  df[col] = pd.to_datetime(df[col], errors='coerce', dayfirst=True)\n",
      "C:\\Users\\hetvi\\AppData\\Local\\Temp\\ipykernel_1808\\3160890175.py:23: UserWarning: Parsing dates in %m/%d/%Y format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  df[col] = pd.to_datetime(df[col], errors='coerce', dayfirst=True)\n",
      "d:\\Make_my_trip\\Recommendation\\recomm\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [14:57:14] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Model Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.41      0.45      2402\n",
      "           1       0.52      0.60      0.56      2526\n",
      "\n",
      "    accuracy                           0.51      4928\n",
      "   macro avg       0.51      0.51      0.50      4928\n",
      "weighted avg       0.51      0.51      0.50      4928\n",
      "\n",
      "Accuracy: 0.5083\n",
      "\n",
      "Random Forest Model Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.41      0.45      2402\n",
      "           1       0.52      0.60      0.56      2526\n",
      "\n",
      "    accuracy                           0.51      4928\n",
      "   macro avg       0.51      0.51      0.50      4928\n",
      "weighted avg       0.51      0.51      0.50      4928\n",
      "\n",
      "Accuracy: 0.5083\n",
      "\n",
      "Decision Tree Model Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.41      0.45      2402\n",
      "           1       0.52      0.60      0.56      2526\n",
      "\n",
      "    accuracy                           0.51      4928\n",
      "   macro avg       0.51      0.51      0.50      4928\n",
      "weighted avg       0.51      0.51      0.50      4928\n",
      "\n",
      "Accuracy: 0.5083\n",
      "\n",
      "Logistic Regression Model Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.23      0.32      2402\n",
      "           1       0.52      0.78      0.62      2526\n",
      "\n",
      "    accuracy                           0.51      4928\n",
      "   macro avg       0.51      0.51      0.47      4928\n",
      "weighted avg       0.51      0.51      0.47      4928\n",
      "\n",
      "Accuracy: 0.5122\n",
      "\n",
      "SVM Model Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.22      0.31      2402\n",
      "           1       0.52      0.80      0.63      2526\n",
      "\n",
      "    accuracy                           0.52      4928\n",
      "   macro avg       0.52      0.51      0.47      4928\n",
      "weighted avg       0.52      0.52      0.47      4928\n",
      "\n",
      "Accuracy: 0.5179\n",
      "\n",
      "Best Model: SVM with Accuracy: 0.5179\n"
     ]
    }
   ],
   "source": [
    "## refining the models and evaluating their performance.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "class RecommendationProcessor:\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self.processed_data = None\n",
    "        self.models = {}\n",
    "        self.best_model = None\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        df = pd.read_csv(self.file_path)\n",
    "        date_columns = [col for col in ['flight_date', 'check_in', 'check_out'] if col in df.columns]\n",
    "        for col in date_columns:\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce', dayfirst=True)\n",
    "        if date_columns:\n",
    "            df.dropna(subset=date_columns, inplace=True)\n",
    "        \n",
    "        df['days'] = (df['check_out'] - df['check_in']).dt.days if 'check_out' in df.columns else 1\n",
    "        df['distance'] = df['distance'].fillna(df['distance'].median()) if 'distance' in df.columns else 100\n",
    "        df['price_per_day'] = df['price_per_day'].fillna(df['price_per_day'].median()) if 'price_per_day' in df.columns else 500\n",
    "        df['recommend_car'] = (\n",
    "            (df['distance'] > df['distance'].median() * 1.1) |  \n",
    "            (df['days'] >= df['days'].median() * 0.9) |  \n",
    "            (df['price_per_day'] > df['price_per_day'].median() * 1.1)\n",
    "        ).astype(int)\n",
    "        \n",
    "        if df['recommend_car'].nunique() == 1:\n",
    "            df['recommend_car'] = np.random.choice([0, 1], size=len(df), p=[0.5, 0.5])\n",
    "        \n",
    "        self.processed_data = df\n",
    "\n",
    "    def train_and_evaluate_models(self):\n",
    "        df = self.processed_data\n",
    "        feature_columns = ['distance', 'days', 'price_per_day']\n",
    "        X = df[[col for col in feature_columns if col in df.columns]]\n",
    "        y = df['recommend_car']\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        models = {\n",
    "            \"XGBoost\": XGBClassifier(n_estimators=100, max_depth=5, learning_rate=0.1, use_label_encoder=False, eval_metric='logloss'),\n",
    "            \"Random Forest\": RandomForestClassifier(n_estimators=200, max_depth=10, min_samples_split=5, random_state=42),\n",
    "            \"Decision Tree\": DecisionTreeClassifier(max_depth=10, min_samples_split=5, random_state=42),\n",
    "            \"Logistic Regression\": LogisticRegression(solver='liblinear', C=1.0),\n",
    "            \"SVM\": SVC(kernel='rbf', C=1.0, gamma='scale')\n",
    "        }\n",
    "\n",
    "        best_accuracy = 0\n",
    "        for name, model in models.items():\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            print(f\"{name} Model Performance:\")\n",
    "            print(classification_report(y_test, y_pred))\n",
    "            print(f\"Accuracy: {accuracy:.4f}\\n\")\n",
    "            self.models[name] = model\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                self.best_model = (name, model)\n",
    "        \n",
    "        print(f\"Best Model: {self.best_model[0]} with Accuracy: {best_accuracy:.4f}\")\n",
    "\n",
    "def main():\n",
    "    processor = RecommendationProcessor(\"D:\\\\Make_my_trip\\\\previous_ds\\\\2.csv\")\n",
    "    processor.preprocess_data()\n",
    "    processor.train_and_evaluate_models()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGBoost...\n",
      "XGBoost Model Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.36      0.42      1597\n",
      "           1       0.51      0.65      0.57      1623\n",
      "\n",
      "    accuracy                           0.51      3220\n",
      "   macro avg       0.50      0.50      0.49      3220\n",
      "weighted avg       0.50      0.51      0.49      3220\n",
      "\n",
      "Accuracy: 0.5053\n",
      "\n",
      "Training RandomForest...\n",
      "RandomForest Model Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.38      0.43      1597\n",
      "           1       0.51      0.63      0.56      1623\n",
      "\n",
      "    accuracy                           0.50      3220\n",
      "   macro avg       0.50      0.50      0.50      3220\n",
      "weighted avg       0.50      0.50      0.50      3220\n",
      "\n",
      "Accuracy: 0.5043\n",
      "\n",
      "Training SVM...\n",
      "SVM Model Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.17      0.25      1597\n",
      "           1       0.50      0.82      0.62      1623\n",
      "\n",
      "    accuracy                           0.50      3220\n",
      "   macro avg       0.50      0.50      0.44      3220\n",
      "weighted avg       0.50      0.50      0.44      3220\n",
      "\n",
      "Accuracy: 0.5009\n",
      "\n",
      "Training KNN...\n",
      "KNN Model Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.50      0.50      1597\n",
      "           1       0.50      0.49      0.50      1623\n",
      "\n",
      "    accuracy                           0.50      3220\n",
      "   macro avg       0.50      0.50      0.50      3220\n",
      "weighted avg       0.50      0.50      0.50      3220\n",
      "\n",
      "Accuracy: 0.4975\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "class RecommendationProcessor:\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self.processed_data = None\n",
    "        self.models = {}\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        df = pd.read_csv(self.file_path)\n",
    "        date_columns = [col for col in ['flight_date', 'check_in', 'check_out'] if col in df.columns]\n",
    "        \n",
    "        for col in date_columns:\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce', format='%d-%m-%Y')\n",
    "        \n",
    "        if date_columns:\n",
    "            df.dropna(subset=date_columns, inplace=True)\n",
    "        \n",
    "        df['days'] = (df['check_out'] - df['check_in']).dt.days if 'check_out' in df.columns else 1\n",
    "        df['distance'] = df['distance'].fillna(df['distance'].median()) if 'distance' in df.columns else 100\n",
    "        df['price_per_day'] = df['price_per_day'].fillna(df['price_per_day'].median()) if 'price_per_day' in df.columns else 500\n",
    "        \n",
    "        df['recommend_car'] = (\n",
    "            (df['distance'] > df['distance'].median() * 1.1) |  \n",
    "            (df['days'] >= df['days'].median() * 0.9) |  \n",
    "            (df['price_per_day'] > df['price_per_day'].median() * 1.1)\n",
    "        ).astype(int)\n",
    "        \n",
    "        if df['recommend_car'].nunique() == 1:\n",
    "            df['recommend_car'] = np.random.choice([0, 1], size=len(df), p=[0.5, 0.5])\n",
    "        \n",
    "        self.processed_data = df\n",
    "\n",
    "    def train_models(self):\n",
    "        df = self.processed_data\n",
    "        feature_columns = ['distance', 'days', 'price_per_day']\n",
    "        X = df[[col for col in feature_columns if col in df.columns]]\n",
    "        y = df['recommend_car']\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        models = {\n",
    "            'XGBoost': XGBClassifier(),\n",
    "            'RandomForest': RandomForestClassifier(),\n",
    "            'SVM': SVC(),\n",
    "            'KNN': KNeighborsClassifier()\n",
    "        }\n",
    "        \n",
    "        tuned_params = {\n",
    "            'XGBoost': {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 0.2]},\n",
    "            'RandomForest': {'n_estimators': [50, 100, 200], 'max_depth': [None, 10, 20]},\n",
    "            'SVM': {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']},\n",
    "            'KNN': {'n_neighbors': [3, 5, 7], 'weights': ['uniform', 'distance']}\n",
    "        }\n",
    "        \n",
    "        for name, model in models.items():\n",
    "            print(f\"Training {name}...\")\n",
    "            search = GridSearchCV(model, tuned_params[name], cv=5, scoring='accuracy', n_jobs=-1)\n",
    "            search.fit(X_train, y_train)\n",
    "            best_model = search.best_estimator_\n",
    "            y_pred = best_model.predict(X_test)\n",
    "            \n",
    "            print(f\"{name} Model Performance:\")\n",
    "            print(classification_report(y_test, y_pred))\n",
    "            print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\\n\")\n",
    "            \n",
    "            self.models[name] = best_model\n",
    "\n",
    "def main():\n",
    "    processor = RecommendationProcessor(\"D:\\\\Make_my_trip\\\\previous_ds\\\\2.csv\")\n",
    "    processor.preprocess_data()\n",
    "    processor.train_models()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using content based filtering for recommendation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Cars for User 5: [['Sedan', 'Sixt'], ['Hatchback', 'Enterprise']]\n",
      "\n",
      "ðŸ”¹ **Precision:** 1.00\n",
      "ðŸ”¹ **Recall:** 0.11\n",
      "ðŸ”¹ **Coverage:** 0.10\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load dataset\n",
    "file_path = \"D:/Make_my_trip/previous_ds/2.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure 'user_id' column exists\n",
    "if 'user_id' not in df.columns:\n",
    "    raise ValueError(\"The dataset must have a 'user_id' column.\")\n",
    "\n",
    "# Filter users who booked flights and hotels but NOT a car\n",
    "users_without_cars = df[(df['flight_price'].notna()) & \n",
    "                        (df['total_hotel'].notna()) & \n",
    "                        (df['carType'].isna())].copy().reset_index(drop=True)\n",
    "\n",
    "# Users who have booked a car (used for recommendations)\n",
    "users_with_cars = df[df['carType'].notna()].copy().reset_index(drop=True)\n",
    "\n",
    "# Fill missing values in feature columns\n",
    "feature_cols = ['pickupLocation', 'dropoffLocation', 'rentalAgency', 'fuelPolicy']\n",
    "users_with_cars[feature_cols] = users_with_cars[feature_cols].fillna(\"Unknown\")\n",
    "users_without_cars[feature_cols] = users_without_cars[feature_cols].fillna(\"Unknown\")\n",
    "\n",
    "# Combine feature columns into a single text feature\n",
    "users_with_cars['combined_features'] = users_with_cars[feature_cols].astype(str).agg(' '.join, axis=1)\n",
    "users_without_cars['combined_features'] = users_without_cars[feature_cols].astype(str).agg(' '.join, axis=1)\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix_with_cars = tfidf.fit_transform(users_with_cars['combined_features'])\n",
    "tfidf_matrix_without_cars = tfidf.transform(users_without_cars['combined_features'])\n",
    "\n",
    "# Compute similarity between users who need cars and those who booked cars\n",
    "cosine_sim = cosine_similarity(tfidf_matrix_without_cars, tfidf_matrix_with_cars)\n",
    "\n",
    "# Function to recommend Top-N similar cars for a user\n",
    "def recommend_car(user_id, top_n=3):\n",
    "    if user_id not in df['user_id'].values:\n",
    "        return f\"User {user_id} does not exist in the dataset.\"\n",
    "\n",
    "    if user_id in users_with_cars['user_id'].values:\n",
    "        return f\"User {user_id} has already booked a car.\"\n",
    "\n",
    "    # Find the user index in users_without_cars\n",
    "    user_row = users_without_cars[users_without_cars['user_id'] == user_id]\n",
    "    \n",
    "    if user_row.empty:\n",
    "        return f\"User {user_id} is not eligible for car recommendations.\"\n",
    "\n",
    "    user_index = user_row.index[0]\n",
    "\n",
    "    # Find the Top-N most similar users who booked a car\n",
    "    top_indices = cosine_sim[user_index].argsort()[-top_n:][::-1]  # Get Top-N matches\n",
    "    \n",
    "    # Get recommended cars and remove duplicates\n",
    "    recommended_cars = users_with_cars.iloc[top_indices][['carType', 'rentalAgency']].drop_duplicates().values\n",
    "\n",
    "    return recommended_cars.tolist()\n",
    "\n",
    "# Evaluate recommendation system\n",
    "def evaluate_recommendations():\n",
    "    correct_recommendations = 0\n",
    "    total_recommendations = 0\n",
    "    unique_recommended_cars = set()\n",
    "\n",
    "    for user_id in users_without_cars['user_id'].unique():\n",
    "        recommended_cars = recommend_car(user_id, top_n=3)\n",
    "\n",
    "        if isinstance(recommended_cars, str):  # Skip if no recommendations\n",
    "            continue\n",
    "\n",
    "        recommended_cars = {tuple(car) for car in recommended_cars}  # Convert to set of tuples\n",
    "        total_recommendations += len(recommended_cars)\n",
    "        unique_recommended_cars.update(recommended_cars)\n",
    "\n",
    "        # Corrected comparison: Check if both carType and rentalAgency match\n",
    "        for car in recommended_cars:\n",
    "            if any((users_with_cars['carType'] == car[0]) & (users_with_cars['rentalAgency'] == car[1])):\n",
    "                correct_recommendations += 1\n",
    "\n",
    "    precision = correct_recommendations / total_recommendations if total_recommendations else 0\n",
    "    recall = correct_recommendations / len(users_with_cars) if len(users_with_cars) else 0\n",
    "    coverage = len(unique_recommended_cars) / len(users_with_cars[['carType', 'rentalAgency']].drop_duplicates()) if len(users_with_cars) else 0\n",
    "\n",
    "    return f\"\\nðŸ”¹ **Precision:** {precision:.2f}\\nðŸ”¹ **Recall:** {recall:.2f}\\nðŸ”¹ **Coverage:** {coverage:.2f}\"\n",
    "\n",
    "# Example: Recommend cars for a user\n",
    "user_id = 5  # Change this to any user ID you want to check\n",
    "print(f\"Recommended Cars for User {user_id}: {recommend_car(user_id)}\")\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(evaluate_recommendations())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Cars for User 5: [['Sedan' 'Sixt']\n",
      " ['Hatchback' 'Enterprise']\n",
      " ['Sedan' 'Sixt']\n",
      " ['Luxury' 'Enterprise']\n",
      " ['Sedan' 'Hertz']\n",
      " ['Sedan' 'Enterprise']\n",
      " ['Hatchback' 'Enterprise']\n",
      " ['Hatchback' 'Enterprise']\n",
      " ['Sedan' 'Hertz']\n",
      " ['SUV' 'Enterprise']]\n",
      "\n",
      "    ðŸ”¹ **Precision:** 0.10\n",
      "    ðŸ”¹ **Recall:** 0.04\n",
      "    ðŸ”¹ **Coverage:** 1.00\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load dataset\n",
    "file_path = \"D:/Make_my_trip/previous_ds/2.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Filter users who booked flights and hotels but NOT a car\n",
    "users_without_cars = df[(df['flight_price'].notna()) & \n",
    "                        (df['total_hotel'].notna()) & \n",
    "                        (df['carType'].isna())].copy()\n",
    "\n",
    "# Users who have booked a car (used for recommendations)\n",
    "users_with_cars = df[df['carType'].notna()].copy()\n",
    "\n",
    "# Fill missing values in feature columns using .loc to avoid warnings\n",
    "feature_cols = ['pickupLocation', 'dropoffLocation', 'rentalAgency', 'fuelPolicy']\n",
    "users_with_cars.loc[:, feature_cols] = users_with_cars[feature_cols].fillna(\"Unknown\")\n",
    "users_without_cars.loc[:, feature_cols] = users_without_cars[feature_cols].fillna(\"Unknown\")\n",
    "\n",
    "# Combine feature columns into a single text feature\n",
    "users_with_cars.loc[:, 'combined_features'] = users_with_cars[feature_cols].astype(str).agg(' '.join, axis=1)\n",
    "users_without_cars.loc[:, 'combined_features'] = users_without_cars[feature_cols].astype(str).agg(' '.join, axis=1)\n",
    "\n",
    "# Reset index to avoid index mismatches\n",
    "users_without_cars.reset_index(drop=True, inplace=True)\n",
    "users_with_cars.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix_with_cars = tfidf.fit_transform(users_with_cars['combined_features'])\n",
    "tfidf_matrix_without_cars = tfidf.transform(users_without_cars['combined_features'])\n",
    "\n",
    "# Compute similarity between users who need cars and those who booked cars\n",
    "cosine_sim = cosine_similarity(tfidf_matrix_without_cars, tfidf_matrix_with_cars)\n",
    "\n",
    "# Function to recommend a car for a specific user (Top-N)\n",
    "def recommend_car(user_id, top_n=3):\n",
    "    if user_id not in users_without_cars['user_id'].values:\n",
    "        return f\"User {user_id} has already booked a car or does not exist in the dataset.\"\n",
    "\n",
    "    # Get user index in users_without_cars\n",
    "    user_index = users_without_cars[users_without_cars['user_id'] == user_id].index[0]\n",
    "\n",
    "    # Ensure index is within valid range\n",
    "    if user_index >= cosine_sim.shape[0]:\n",
    "        return f\"Error: User index {user_index} is out of range.\"\n",
    "\n",
    "    # Find the Top-N most similar users who booked a car\n",
    "    top_indices = cosine_sim[user_index].argsort()[-top_n:][::-1]\n",
    "\n",
    "    # Ensure indices are valid\n",
    "    top_indices = [idx for idx in top_indices if idx < len(users_with_cars)]\n",
    "\n",
    "    # Get recommended cars\n",
    "    recommended_cars = users_with_cars.iloc[top_indices][['carType', 'rentalAgency']].values\n",
    "\n",
    "    return recommended_cars if recommended_cars.size > 0 else \"No suitable recommendations found.\"\n",
    "\n",
    "# Function to evaluate recommendations\n",
    "def evaluate_recommendations(top_n=3):\n",
    "    total_users = len(users_without_cars)\n",
    "    successful_recommendations = 0\n",
    "    total_recommendations = 0\n",
    "    unique_recommended_cars = set()\n",
    "    unique_actual_cars = set(users_with_cars['carType'].unique())\n",
    "\n",
    "    for user_id in users_without_cars['user_id'].unique():\n",
    "        top_cars = recommend_car(user_id, top_n)\n",
    "\n",
    "        if isinstance(top_cars, str):  # Skip if no recommendations\n",
    "            continue\n",
    "\n",
    "        total_recommendations += len(top_cars)\n",
    "        unique_recommended_cars.update([car[0] for car in top_cars])\n",
    "\n",
    "        # Check if any recommended car matches the actual booked cars\n",
    "        if any(car[0] in unique_actual_cars for car in top_cars):\n",
    "            successful_recommendations += 1\n",
    "\n",
    "    # Compute evaluation metrics\n",
    "    precision = successful_recommendations / total_recommendations if total_recommendations > 0 else 0\n",
    "    recall = successful_recommendations / total_users if total_users > 0 else 0\n",
    "    coverage = len(unique_recommended_cars) / len(unique_actual_cars) if len(unique_actual_cars) > 0 else 0\n",
    "\n",
    "    return f\"\"\"\n",
    "    ðŸ”¹ **Precision:** {precision:.2f}\n",
    "    ðŸ”¹ **Recall:** {recall:.2f}\n",
    "    ðŸ”¹ **Coverage:** {coverage:.2f}\n",
    "    \"\"\"\n",
    "\n",
    "# Example: Recommend a car for user with ID 5\n",
    "user_id = 5\n",
    "top_n = 10  # Change this to Top-N recommendations\n",
    "top_cars = recommend_car(user_id, top_n)\n",
    "print(f\"Recommended Cars for User {user_id}: {top_cars}\")\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(evaluate_recommendations(top_n=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### recommendation considering the travelCode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Cars for User 0 (Travel Code 2): [['Sedan' 'Sixt']\n",
      " ['Hatchback' 'Enterprise']\n",
      " ['Sedan' 'Sixt']\n",
      " ['Luxury' 'Enterprise']\n",
      " ['Sedan' 'Hertz']\n",
      " ['Sedan' 'Enterprise']\n",
      " ['Hatchback' 'Enterprise']\n",
      " ['Hatchback' 'Enterprise']\n",
      " ['Sedan' 'Hertz']\n",
      " ['SUV' 'Enterprise']]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load dataset\n",
    "file_path = \"D:/Make_my_trip/previous_ds/2.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Filter users who booked flights and hotels but NOT a car\n",
    "users_without_cars = df[(df['flight_price'].notna()) & \n",
    "                        (df['total_hotel'].notna()) & \n",
    "                        (df['carType'].isna())].copy()\n",
    "\n",
    "# Users who have booked a car (used for recommendations)\n",
    "users_with_cars = df[df['carType'].notna()].copy()\n",
    "\n",
    "# Fill missing values in feature columns\n",
    "feature_cols = ['pickupLocation', 'dropoffLocation', 'rentalAgency', 'fuelPolicy']\n",
    "users_with_cars[feature_cols] = users_with_cars[feature_cols].fillna(\"Unknown\")\n",
    "users_without_cars[feature_cols] = users_without_cars[feature_cols].fillna(\"Unknown\")\n",
    "\n",
    "# Add travelCode to the combined feature set\n",
    "users_with_cars['combined_features'] = users_with_cars[feature_cols + ['travelCode']].astype(str).agg(' '.join, axis=1)\n",
    "users_without_cars['combined_features'] = users_without_cars[feature_cols + ['travelCode']].astype(str).agg(' '.join, axis=1)\n",
    "\n",
    "# Reset index\n",
    "users_without_cars.reset_index(drop=True, inplace=True)\n",
    "users_with_cars.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix_with_cars = tfidf.fit_transform(users_with_cars['combined_features'])\n",
    "tfidf_matrix_without_cars = tfidf.transform(users_without_cars['combined_features'])\n",
    "\n",
    "# Compute similarity between users who need cars and those who booked cars\n",
    "cosine_sim = cosine_similarity(tfidf_matrix_without_cars, tfidf_matrix_with_cars)\n",
    "\n",
    "# Function to recommend a car with fallback option\n",
    "def recommend_car(user_id, travel_code, top_n=3):\n",
    "    user_record = users_without_cars[(users_without_cars['user_id'] == user_id) & (users_without_cars['travelCode'] == travel_code)]\n",
    "    if user_record.empty:\n",
    "        return f\"User {user_id} with Travel Code {travel_code} has already booked a car or does not exist in the dataset.\"\n",
    "\n",
    "    user_index = user_record.index[0]\n",
    "    if user_index >= cosine_sim.shape[0]:\n",
    "        return f\"Error: User index {user_index} is out of range.\"\n",
    "\n",
    "    # Find top-N most similar users who booked a car (exact travelCode match first)\n",
    "    top_indices = cosine_sim[user_index].argsort()[-top_n:][::-1]\n",
    "    travel_matches = users_with_cars.iloc[top_indices]\n",
    "    travel_matches = travel_matches[travel_matches['travelCode'] == travel_code]\n",
    "    \n",
    "    if not travel_matches.empty:\n",
    "        return travel_matches[['carType', 'rentalAgency']].values\n",
    "    \n",
    "    # Fallback: Recommend based on location & rental agency if no exact travelCode match\n",
    "    location_matches = users_with_cars.iloc[top_indices]\n",
    "    return location_matches[['carType', 'rentalAgency']].values if not location_matches.empty else \"No suitable recommendations found.\"\n",
    "\n",
    "# Example: Recommend a car for a specific user & travelCode\n",
    "user_id = 0\n",
    "travel_code = 2  # Replace with actual travel code from dataset\n",
    "top_n = 10  # Change this to Top-N recommendations\n",
    "top_cars = recommend_car(user_id, travel_code, top_n)\n",
    "print(f\"Recommended Cars for User {user_id} (Travel Code {travel_code}): {top_cars}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Cars for User 0 (Travel Code 2): [['Sedan' 'Enterprise']\n",
      " ['Hatchback' 'Enterprise']\n",
      " ['SUV' 'Enterprise']\n",
      " ['Luxury' 'Enterprise']\n",
      " ['SUV' 'Avis']\n",
      " ['Luxury' 'Sixt']\n",
      " ['SUV' 'Enterprise']]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 100\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecommended Cars for User \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (Travel Code \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtravel_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtop_cars\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# Evaluate system\u001b[39;00m\n\u001b[1;32m--> 100\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mevaluate_recommendations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtop_n\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[19], line 77\u001b[0m, in \u001b[0;36mevaluate_recommendations\u001b[1;34m(top_n)\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     76\u001b[0m total_recommendations \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(top_cars)\n\u001b[1;32m---> 77\u001b[0m unique_recommended_cars\u001b[38;5;241m.\u001b[39mupdate([car[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m car \u001b[38;5;129;01min\u001b[39;00m top_cars])\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(car[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m unique_actual_cars \u001b[38;5;28;01mfor\u001b[39;00m car \u001b[38;5;129;01min\u001b[39;00m top_cars):\n\u001b[0;32m     79\u001b[0m     successful_recommendations \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load dataset\n",
    "file_path = \"D:/Make_my_trip/previous_ds/2.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Filter users who booked flights and hotels but NOT a car\n",
    "users_without_cars = df[(df['flight_price'].notna()) & \n",
    "                        (df['total_hotel'].notna()) & \n",
    "                        (df['carType'].isna())].copy()\n",
    "\n",
    "# Users who have booked a car (used for recommendations)\n",
    "users_with_cars = df[df['carType'].notna()].copy()\n",
    "\n",
    "# Fill missing values in feature columns\n",
    "feature_cols = ['pickupLocation', 'dropoffLocation', 'rentalAgency', 'fuelPolicy']\n",
    "users_with_cars[feature_cols] = users_with_cars[feature_cols].fillna(\"Unknown\")\n",
    "users_without_cars[feature_cols] = users_without_cars[feature_cols].fillna(\"Unknown\")\n",
    "\n",
    "# Combine feature columns for similarity matching\n",
    "users_with_cars['combined_features'] = users_with_cars[feature_cols + ['travelCode']].astype(str).agg(' '.join, axis=1)\n",
    "users_without_cars['combined_features'] = users_without_cars[feature_cols + ['travelCode']].astype(str).agg(' '.join, axis=1)\n",
    "\n",
    "# Reset index\n",
    "users_without_cars.reset_index(drop=True, inplace=True)\n",
    "users_with_cars.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix_with_cars = tfidf.fit_transform(users_with_cars['combined_features'])\n",
    "tfidf_matrix_without_cars = tfidf.transform(users_without_cars['combined_features'])\n",
    "\n",
    "# Compute cosine similarity\n",
    "cosine_sim = cosine_similarity(tfidf_matrix_without_cars, tfidf_matrix_with_cars)\n",
    "\n",
    "# Function to recommend cars\n",
    "def recommend_car(user_id, travel_code, top_n=3):\n",
    "    user_record = users_without_cars[(users_without_cars['user_id'] == user_id) & (users_without_cars['travelCode'] == travel_code)]\n",
    "    if user_record.empty:\n",
    "        return f\"User {user_id} with Travel Code {travel_code} has already booked a car or does not exist in the dataset.\"\n",
    "    \n",
    "    user_index = user_record.index[0]\n",
    "    top_indices = cosine_sim[user_index].argsort()[-top_n:][::-1]\n",
    "    recommended_cars = users_with_cars.iloc[top_indices]\n",
    "    recommended_cars = recommended_cars[recommended_cars['travelCode'] == travel_code][['carType', 'rentalAgency']].values\n",
    "    \n",
    "    # If no exact match, find similar travel codes\n",
    "    if recommended_cars.size == 0:\n",
    "        travel_code_similarity = cosine_similarity(tfidf_matrix_with_cars, tfidf_matrix_with_cars)\n",
    "        similar_travel_codes = users_with_cars.iloc[travel_code_similarity[user_index].argsort()[-5:][::-1]]['travelCode'].unique()\n",
    "        \n",
    "        recommended_cars = users_with_cars[users_with_cars['travelCode'].isin(similar_travel_codes)][['carType', 'rentalAgency']].values\n",
    "    \n",
    "    # If still no match, recommend most popular cars\n",
    "    if recommended_cars.size == 0:\n",
    "        popular_cars = users_with_cars.groupby(['carType', 'rentalAgency']).size().reset_index(name='count')\n",
    "        recommended_cars = popular_cars.sort_values(by='count', ascending=False).head(top_n)[['carType', 'rentalAgency']].values\n",
    "    \n",
    "    return recommended_cars if recommended_cars.size > 0 else \"No suitable recommendations found.\"\n",
    "\n",
    "# Function to evaluate recommendations\n",
    "def evaluate_recommendations(top_n=3):\n",
    "    total_users = len(users_without_cars)\n",
    "    successful_recommendations = 0\n",
    "    total_recommendations = 0\n",
    "    unique_recommended_cars = set()\n",
    "    unique_actual_cars = set(users_with_cars['carType'].unique())\n",
    "\n",
    "    for _, row in users_without_cars.iterrows():\n",
    "        user_id, travel_code = row['user_id'], row['travelCode']\n",
    "        top_cars = recommend_car(user_id, travel_code, top_n)\n",
    "        if isinstance(top_cars, str):  # Skip if no recommendations\n",
    "            continue\n",
    "        total_recommendations += len(top_cars)\n",
    "        unique_recommended_cars.update([car[0] for car in top_cars])\n",
    "        if any(car[0] in unique_actual_cars for car in top_cars):\n",
    "            successful_recommendations += 1\n",
    "    \n",
    "    precision = successful_recommendations / total_recommendations if total_recommendations > 0 else 0\n",
    "    recall = successful_recommendations / total_users if total_users > 0 else 0\n",
    "    coverage = len(unique_recommended_cars) / len(unique_actual_cars) if len(unique_actual_cars) > 0 else 0\n",
    "\n",
    "    return f\"\"\"\n",
    "    ðŸ”¹ **Precision:** {precision:.2f}\n",
    "    ðŸ”¹ **Recall:** {recall:.2f}\n",
    "    ðŸ”¹ **Coverage:** {coverage:.2f}\n",
    "    \"\"\"\n",
    "\n",
    "# Example: Recommend a car for a user\n",
    "user_id = 0  # Replace with an actual user ID\n",
    "travel_code = 2  # Replace with actual travel code\n",
    "top_n = 10\n",
    "\n",
    "top_cars = recommend_car(user_id, travel_code, top_n)\n",
    "print(f\"Recommended Cars for User {user_id} (Travel Code {travel_code}): {top_cars}\")\n",
    "\n",
    "# Evaluate system\n",
    "print(evaluate_recommendations(top_n=10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Cars for User 0 (Travel Code 39): [['Sedan' 'Sixt']\n",
      " ['Hatchback' 'Enterprise']\n",
      " ['Sedan' 'Sixt']\n",
      " ['Luxury' 'Enterprise']\n",
      " ['Sedan' 'Hertz']\n",
      " ['Sedan' 'Enterprise']\n",
      " ['Hatchback' 'Enterprise']\n",
      " ['Hatchback' 'Enterprise']\n",
      " ['Sedan' 'Hertz']\n",
      " ['SUV' 'Enterprise']]\n",
      "Processed 0/36779 users...\n",
      "Processed 100/36779 users...\n",
      "Processed 200/36779 users...\n",
      "Processed 300/36779 users...\n",
      "Processed 400/36779 users...\n",
      "Processed 500/36779 users...\n",
      "Processed 600/36779 users...\n",
      "Processed 700/36779 users...\n",
      "Processed 800/36779 users...\n",
      "Processed 900/36779 users...\n",
      "Processed 1000/36779 users...\n",
      "Processed 1100/36779 users...\n",
      "Processed 1200/36779 users...\n",
      "Processed 1300/36779 users...\n",
      "Processed 1400/36779 users...\n",
      "Processed 1500/36779 users...\n",
      "Processed 1600/36779 users...\n",
      "Processed 1700/36779 users...\n",
      "Processed 1800/36779 users...\n",
      "Processed 1900/36779 users...\n",
      "Processed 2000/36779 users...\n",
      "Processed 2100/36779 users...\n",
      "Processed 2200/36779 users...\n",
      "Processed 2300/36779 users...\n",
      "Processed 2400/36779 users...\n",
      "Processed 2500/36779 users...\n",
      "Processed 2600/36779 users...\n",
      "Processed 2700/36779 users...\n",
      "Processed 2800/36779 users...\n",
      "Processed 2900/36779 users...\n",
      "Processed 3000/36779 users...\n",
      "Processed 3100/36779 users...\n",
      "Processed 3200/36779 users...\n",
      "Processed 3300/36779 users...\n",
      "Processed 3400/36779 users...\n",
      "Processed 3500/36779 users...\n",
      "Processed 3600/36779 users...\n",
      "Processed 3700/36779 users...\n",
      "Processed 3800/36779 users...\n",
      "Processed 3900/36779 users...\n",
      "Processed 4000/36779 users...\n",
      "Processed 4100/36779 users...\n",
      "Processed 4200/36779 users...\n",
      "Processed 4300/36779 users...\n",
      "Processed 4400/36779 users...\n",
      "Processed 4500/36779 users...\n",
      "Processed 4600/36779 users...\n",
      "Processed 4700/36779 users...\n",
      "Processed 4800/36779 users...\n",
      "Processed 4900/36779 users...\n",
      "Processed 5000/36779 users...\n",
      "Processed 5100/36779 users...\n",
      "Processed 5200/36779 users...\n",
      "Processed 5300/36779 users...\n",
      "Processed 5400/36779 users...\n",
      "Processed 5500/36779 users...\n",
      "Processed 5600/36779 users...\n",
      "Processed 5700/36779 users...\n",
      "Processed 5800/36779 users...\n",
      "Processed 5900/36779 users...\n",
      "Processed 6000/36779 users...\n",
      "Processed 6100/36779 users...\n",
      "Processed 6200/36779 users...\n",
      "Processed 6300/36779 users...\n",
      "Processed 6400/36779 users...\n",
      "Processed 6500/36779 users...\n",
      "Processed 6600/36779 users...\n",
      "Processed 6700/36779 users...\n",
      "Processed 6800/36779 users...\n",
      "Processed 6900/36779 users...\n",
      "Processed 7000/36779 users...\n",
      "Processed 7100/36779 users...\n",
      "Processed 7200/36779 users...\n",
      "Processed 7300/36779 users...\n",
      "Processed 7400/36779 users...\n",
      "Processed 7500/36779 users...\n",
      "Processed 7600/36779 users...\n",
      "Processed 7700/36779 users...\n",
      "Processed 7800/36779 users...\n",
      "Processed 7900/36779 users...\n",
      "Processed 8000/36779 users...\n",
      "Processed 8100/36779 users...\n",
      "Processed 8200/36779 users...\n",
      "Processed 8300/36779 users...\n",
      "Processed 8400/36779 users...\n",
      "Processed 8500/36779 users...\n",
      "Processed 8600/36779 users...\n",
      "Processed 8700/36779 users...\n",
      "Processed 8800/36779 users...\n",
      "Processed 8900/36779 users...\n",
      "Processed 9000/36779 users...\n",
      "Processed 9100/36779 users...\n",
      "Processed 9200/36779 users...\n",
      "Processed 9300/36779 users...\n",
      "Processed 9400/36779 users...\n",
      "Processed 9500/36779 users...\n",
      "Processed 9600/36779 users...\n",
      "Processed 9700/36779 users...\n",
      "Processed 9800/36779 users...\n",
      "Processed 9900/36779 users...\n",
      "Processed 10000/36779 users...\n",
      "Processed 10100/36779 users...\n",
      "Processed 10200/36779 users...\n",
      "Processed 10300/36779 users...\n",
      "Processed 10400/36779 users...\n",
      "Processed 10500/36779 users...\n",
      "Processed 10600/36779 users...\n",
      "Processed 10700/36779 users...\n",
      "Processed 10800/36779 users...\n",
      "Processed 10900/36779 users...\n",
      "Processed 11000/36779 users...\n",
      "Processed 11100/36779 users...\n",
      "Processed 11200/36779 users...\n",
      "Processed 11300/36779 users...\n",
      "Processed 11400/36779 users...\n",
      "Processed 11500/36779 users...\n",
      "Processed 11600/36779 users...\n",
      "Processed 11700/36779 users...\n",
      "Processed 11800/36779 users...\n",
      "Processed 11900/36779 users...\n",
      "Processed 12000/36779 users...\n",
      "Processed 12100/36779 users...\n",
      "Processed 12200/36779 users...\n",
      "Processed 12300/36779 users...\n",
      "Processed 12400/36779 users...\n",
      "Processed 12500/36779 users...\n",
      "Processed 12600/36779 users...\n",
      "Processed 12700/36779 users...\n",
      "Processed 12800/36779 users...\n",
      "Processed 12900/36779 users...\n",
      "Processed 13000/36779 users...\n",
      "Processed 13100/36779 users...\n",
      "Processed 13200/36779 users...\n",
      "Processed 13300/36779 users...\n",
      "Processed 13400/36779 users...\n",
      "Processed 13500/36779 users...\n",
      "Processed 13600/36779 users...\n",
      "Processed 13700/36779 users...\n",
      "Processed 13800/36779 users...\n",
      "Processed 13900/36779 users...\n",
      "Processed 14000/36779 users...\n",
      "Processed 14100/36779 users...\n",
      "Processed 14200/36779 users...\n",
      "Processed 14300/36779 users...\n",
      "Processed 14400/36779 users...\n",
      "Processed 14500/36779 users...\n",
      "Processed 14600/36779 users...\n",
      "Processed 14700/36779 users...\n",
      "Processed 14800/36779 users...\n",
      "Processed 14900/36779 users...\n",
      "Processed 15000/36779 users...\n",
      "Processed 15100/36779 users...\n",
      "Processed 15200/36779 users...\n",
      "Processed 15300/36779 users...\n",
      "Processed 15400/36779 users...\n",
      "Processed 15500/36779 users...\n",
      "Processed 15600/36779 users...\n",
      "Processed 15700/36779 users...\n",
      "Processed 15800/36779 users...\n",
      "Processed 15900/36779 users...\n",
      "Processed 16000/36779 users...\n",
      "Processed 16100/36779 users...\n",
      "Processed 16200/36779 users...\n",
      "Processed 16300/36779 users...\n",
      "Processed 16400/36779 users...\n",
      "Processed 16500/36779 users...\n",
      "Processed 16600/36779 users...\n",
      "Processed 16700/36779 users...\n",
      "Processed 16800/36779 users...\n",
      "Processed 16900/36779 users...\n",
      "Processed 17000/36779 users...\n",
      "Processed 17100/36779 users...\n",
      "Processed 17200/36779 users...\n",
      "Processed 17300/36779 users...\n",
      "Processed 17400/36779 users...\n",
      "Processed 17500/36779 users...\n",
      "Processed 17600/36779 users...\n",
      "Processed 17700/36779 users...\n",
      "Processed 17800/36779 users...\n",
      "Processed 17900/36779 users...\n",
      "Processed 18000/36779 users...\n",
      "Processed 18100/36779 users...\n",
      "Processed 18200/36779 users...\n",
      "Processed 18300/36779 users...\n",
      "Processed 18400/36779 users...\n",
      "Processed 18500/36779 users...\n",
      "Processed 18600/36779 users...\n",
      "Processed 18700/36779 users...\n",
      "Processed 18800/36779 users...\n",
      "Processed 18900/36779 users...\n",
      "Processed 19000/36779 users...\n",
      "Processed 19100/36779 users...\n",
      "Processed 19200/36779 users...\n",
      "Processed 19300/36779 users...\n",
      "Processed 19400/36779 users...\n",
      "Processed 19500/36779 users...\n",
      "Processed 19600/36779 users...\n",
      "Processed 19700/36779 users...\n",
      "Processed 19800/36779 users...\n",
      "Processed 19900/36779 users...\n",
      "Processed 20000/36779 users...\n",
      "Processed 20100/36779 users...\n",
      "Processed 20200/36779 users...\n",
      "Processed 20300/36779 users...\n",
      "Processed 20400/36779 users...\n",
      "Processed 20500/36779 users...\n",
      "Processed 20600/36779 users...\n",
      "Processed 20700/36779 users...\n",
      "Processed 20800/36779 users...\n",
      "Processed 20900/36779 users...\n",
      "Processed 21000/36779 users...\n",
      "Processed 21100/36779 users...\n",
      "Processed 21200/36779 users...\n",
      "Processed 21300/36779 users...\n",
      "Processed 21400/36779 users...\n",
      "Processed 21500/36779 users...\n",
      "Processed 21600/36779 users...\n",
      "Processed 21700/36779 users...\n",
      "Processed 21800/36779 users...\n",
      "Processed 21900/36779 users...\n",
      "Processed 22000/36779 users...\n",
      "Processed 22100/36779 users...\n",
      "Processed 22200/36779 users...\n",
      "Processed 22300/36779 users...\n",
      "Processed 22400/36779 users...\n",
      "Processed 22500/36779 users...\n",
      "Processed 22600/36779 users...\n",
      "Processed 22700/36779 users...\n",
      "Processed 22800/36779 users...\n",
      "Processed 22900/36779 users...\n",
      "Processed 23000/36779 users...\n",
      "Processed 23100/36779 users...\n",
      "Processed 23200/36779 users...\n",
      "Processed 23300/36779 users...\n",
      "Processed 23400/36779 users...\n",
      "Processed 23500/36779 users...\n",
      "Processed 23600/36779 users...\n",
      "Processed 23700/36779 users...\n",
      "Processed 23800/36779 users...\n",
      "Processed 23900/36779 users...\n",
      "Processed 24000/36779 users...\n",
      "Processed 24100/36779 users...\n",
      "Processed 24200/36779 users...\n",
      "Processed 24300/36779 users...\n",
      "Processed 24400/36779 users...\n",
      "Processed 24500/36779 users...\n",
      "Processed 24600/36779 users...\n",
      "Processed 24700/36779 users...\n",
      "Processed 24800/36779 users...\n",
      "Processed 24900/36779 users...\n",
      "Processed 25000/36779 users...\n",
      "Processed 25100/36779 users...\n",
      "Processed 25200/36779 users...\n",
      "Processed 25300/36779 users...\n",
      "Processed 25400/36779 users...\n",
      "Processed 25500/36779 users...\n",
      "Processed 25600/36779 users...\n",
      "Processed 25700/36779 users...\n",
      "Processed 25800/36779 users...\n",
      "Processed 25900/36779 users...\n",
      "Processed 26000/36779 users...\n",
      "Processed 26100/36779 users...\n",
      "Processed 26200/36779 users...\n",
      "Processed 26300/36779 users...\n",
      "Processed 26400/36779 users...\n",
      "Processed 26500/36779 users...\n",
      "Processed 26600/36779 users...\n",
      "Processed 26700/36779 users...\n",
      "Processed 26800/36779 users...\n",
      "Processed 26900/36779 users...\n",
      "Processed 27000/36779 users...\n",
      "Processed 27100/36779 users...\n",
      "Processed 27200/36779 users...\n",
      "Processed 27300/36779 users...\n",
      "Processed 27400/36779 users...\n",
      "Processed 27500/36779 users...\n",
      "Processed 27600/36779 users...\n",
      "Processed 27700/36779 users...\n",
      "Processed 27800/36779 users...\n",
      "Processed 27900/36779 users...\n",
      "Processed 28000/36779 users...\n",
      "Processed 28100/36779 users...\n",
      "Processed 28200/36779 users...\n",
      "Processed 28300/36779 users...\n",
      "Processed 28400/36779 users...\n",
      "Processed 28500/36779 users...\n",
      "Processed 28600/36779 users...\n",
      "Processed 28700/36779 users...\n",
      "Processed 28800/36779 users...\n",
      "Processed 28900/36779 users...\n",
      "Processed 29000/36779 users...\n",
      "Processed 29100/36779 users...\n",
      "Processed 29200/36779 users...\n",
      "Processed 29300/36779 users...\n",
      "Processed 29400/36779 users...\n",
      "Processed 29500/36779 users...\n",
      "Processed 29600/36779 users...\n",
      "Processed 29700/36779 users...\n",
      "Processed 29800/36779 users...\n",
      "Processed 29900/36779 users...\n",
      "Processed 30000/36779 users...\n",
      "Processed 30100/36779 users...\n",
      "Processed 30200/36779 users...\n",
      "Processed 30300/36779 users...\n",
      "Processed 30400/36779 users...\n",
      "Processed 30500/36779 users...\n",
      "Processed 30600/36779 users...\n",
      "Processed 30700/36779 users...\n",
      "Processed 30800/36779 users...\n",
      "Processed 30900/36779 users...\n",
      "Processed 31000/36779 users...\n",
      "Processed 31100/36779 users...\n",
      "Processed 31200/36779 users...\n",
      "Processed 31300/36779 users...\n",
      "Processed 31400/36779 users...\n",
      "Processed 31500/36779 users...\n",
      "Processed 31600/36779 users...\n",
      "Processed 31700/36779 users...\n",
      "Processed 31800/36779 users...\n",
      "Processed 31900/36779 users...\n",
      "Processed 32000/36779 users...\n",
      "Processed 32100/36779 users...\n",
      "Processed 32200/36779 users...\n",
      "Processed 32300/36779 users...\n",
      "Processed 32400/36779 users...\n",
      "Processed 32500/36779 users...\n",
      "Processed 32600/36779 users...\n",
      "Processed 32700/36779 users...\n",
      "Processed 32800/36779 users...\n",
      "Processed 32900/36779 users...\n",
      "Processed 33000/36779 users...\n",
      "Processed 33100/36779 users...\n",
      "Processed 33200/36779 users...\n",
      "Processed 33300/36779 users...\n",
      "Processed 33400/36779 users...\n",
      "Processed 33500/36779 users...\n",
      "Processed 33600/36779 users...\n",
      "Processed 33700/36779 users...\n",
      "Processed 33800/36779 users...\n",
      "Processed 33900/36779 users...\n",
      "Processed 34000/36779 users...\n",
      "Processed 34100/36779 users...\n",
      "Processed 34200/36779 users...\n",
      "Processed 34300/36779 users...\n",
      "Processed 34400/36779 users...\n",
      "Processed 34500/36779 users...\n",
      "Processed 34600/36779 users...\n",
      "Processed 34700/36779 users...\n",
      "Processed 34800/36779 users...\n",
      "Processed 34900/36779 users...\n",
      "Processed 35000/36779 users...\n",
      "Processed 35100/36779 users...\n",
      "Processed 35200/36779 users...\n",
      "Processed 35300/36779 users...\n",
      "Processed 35400/36779 users...\n",
      "Processed 35500/36779 users...\n",
      "Processed 35600/36779 users...\n",
      "Processed 35700/36779 users...\n",
      "Processed 35800/36779 users...\n",
      "Processed 35900/36779 users...\n",
      "Processed 36000/36779 users...\n",
      "Processed 36100/36779 users...\n",
      "Processed 36200/36779 users...\n",
      "Processed 36300/36779 users...\n",
      "Processed 36400/36779 users...\n",
      "Processed 36500/36779 users...\n",
      "Processed 36600/36779 users...\n",
      "Processed 36700/36779 users...\n",
      "\n",
      "    ðŸ”¹ **Precision:** 0.10\n",
      "    ðŸ”¹ **Recall:** 1.00\n",
      "    ðŸ”¹ **Coverage:** 1.00\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load dataset\n",
    "file_path = \"D:/Make_my_trip/previous_ds/2.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Filter users who booked flights and hotels but NOT a car\n",
    "users_without_cars = df[(df['flight_price'].notna()) & \n",
    "                        (df['total_hotel'].notna()) & \n",
    "                        (df['carType'].isna())].copy()\n",
    "\n",
    "# Users who have booked a car (used for recommendations)\n",
    "users_with_cars = df[df['carType'].notna()].copy()\n",
    "\n",
    "# Fill missing values in feature columns\n",
    "# feature_cols = ['pickupLocation', 'dropoffLocation', 'rentalAgency', 'fuelPolicy']\n",
    "feature_cols = ['pickupLocation', 'dropoffLocation']\n",
    "\n",
    "users_with_cars[feature_cols] = users_with_cars[feature_cols].fillna(\"Unknown\")\n",
    "users_without_cars[feature_cols] = users_without_cars[feature_cols].fillna(\"Unknown\")\n",
    "\n",
    "# Add travelCode to the combined feature set\n",
    "users_with_cars['combined_features'] = users_with_cars[feature_cols + ['travelCode']].astype(str).agg(' '.join, axis=1)\n",
    "users_without_cars['combined_features'] = users_without_cars[feature_cols + ['travelCode']].astype(str).agg(' '.join, axis=1)\n",
    "\n",
    "# Reset index\n",
    "users_without_cars.reset_index(drop=True, inplace=True)\n",
    "users_with_cars.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# common_travel_codes = set(users_without_cars['travelCode']).intersection(set(users_with_cars['travelCode']))\n",
    "# print(\"Common Travel Codes:\", common_travel_codes)\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix_with_cars = tfidf.fit_transform(users_with_cars['combined_features'])\n",
    "tfidf_matrix_without_cars = tfidf.transform(users_without_cars['combined_features'])\n",
    "\n",
    "# Precompute cosine similarity between all users\n",
    "cosine_sim = cosine_similarity(tfidf_matrix_without_cars, tfidf_matrix_with_cars)\n",
    "\n",
    "# Group users_with_cars by travelCode for faster lookup\n",
    "cars_by_travel_code = users_with_cars.groupby('travelCode')\n",
    "\n",
    "# Convert DataFrame to NumPy arrays for faster lookups\n",
    "user_ids_without_cars = users_without_cars['user_id'].to_numpy()\n",
    "travel_codes_without_cars = users_without_cars['travelCode'].to_numpy()\n",
    "\n",
    "# Function to recommend a car for a specific user (Top-N) per travelCode\n",
    "def recommend_car(user_id, travel_code, top_n=3):\n",
    "    user_record = users_without_cars[users_without_cars['user_id'] == user_id]\n",
    "\n",
    "    if user_record.empty:\n",
    "        print(f\"âŒ No entry found for User {user_id}.\")\n",
    "        return \"No suitable recommendations found.\"\n",
    "\n",
    "    # Find similar users (ignoring exact travelCode match)\n",
    "    user_index = user_record.index[0]\n",
    "    top_indices = cosine_sim[user_index].argsort()[-top_n:][::-1]\n",
    "\n",
    "    recommended_cars = users_with_cars.iloc[top_indices][['carType', 'rentalAgency']].values\n",
    "\n",
    "    if recommended_cars.size == 0:\n",
    "        print(f\"âš ï¸ No cars found for similar users.\")\n",
    "    \n",
    "    return recommended_cars if recommended_cars.size > 0 else \"No suitable recommendations found.\"\n",
    "\n",
    "    # user_indices = np.where((user_ids_without_cars == user_id) & (travel_codes_without_cars == travel_code))[0]\n",
    "    \n",
    "    # if len(user_indices) == 0:\n",
    "    #     return f\"User {user_id} with Travel Code {travel_code} has already booked a car or does not exist in the dataset.\"\n",
    "\n",
    "    # user_index = user_indices[0]\n",
    "\n",
    "    # # Find the Top-N most similar users who booked a car\n",
    "    # top_indices = cosine_sim[user_index].argsort()[-top_n:][::-1]\n",
    "    # recommended_cars = users_with_cars.iloc[top_indices][['carType', 'rentalAgency']].values\n",
    "    # # Get recommended cars for this travelCode\n",
    "    # if travel_code in cars_by_travel_code.groups:\n",
    "    #     recommended_cars = users_with_cars.iloc[top_indices]\n",
    "    #     recommended_cars = recommended_cars[recommended_cars['travelCode'] == travel_code][['carType', 'rentalAgency']].values\n",
    "    #     return recommended_cars if recommended_cars.size > 0 else \"No suitable recommendations found.\"\n",
    "    \n",
    "    # return \"No suitable recommendations found.\"\n",
    "\n",
    "\n",
    "# Function to evaluate recommendations\n",
    "def evaluate_recommendations(top_n=3):\n",
    "    total_users = len(users_without_cars)\n",
    "    successful_recommendations = 0\n",
    "    total_recommendations = 0\n",
    "    unique_recommended_cars = set()\n",
    "    unique_actual_cars = set(users_with_cars['carType'].unique())\n",
    "\n",
    "    for user_index, (user_id, travel_code) in enumerate(zip(user_ids_without_cars, travel_codes_without_cars)):\n",
    "        top_cars = recommend_car(user_id, travel_code, top_n)\n",
    "\n",
    "        if isinstance(top_cars, str):  # Skip if no recommendations\n",
    "            continue\n",
    "\n",
    "        total_recommendations += len(top_cars)\n",
    "        unique_recommended_cars.update([car[0] for car in top_cars])\n",
    "\n",
    "        # Check if any recommended car matches the actual booked cars\n",
    "        if any(car[0] in unique_actual_cars for car in top_cars):\n",
    "            successful_recommendations += 1\n",
    "\n",
    "        # Print progress every 100 users\n",
    "        if user_index % 100 == 0:\n",
    "            print(f\"Processed {user_index}/{total_users} users...\")\n",
    "\n",
    "    # Compute evaluation metrics\n",
    "    precision = successful_recommendations / total_recommendations if total_recommendations > 0 else 0\n",
    "    recall = successful_recommendations / total_users if total_users > 0 else 0\n",
    "    coverage = len(unique_recommended_cars) / len(unique_actual_cars) if len(unique_actual_cars) > 0 else 0\n",
    "\n",
    "    return f\"\"\"\n",
    "    ðŸ”¹ **Precision:** {precision:.2f}\n",
    "    ðŸ”¹ **Recall:** {recall:.2f}\n",
    "    ðŸ”¹ **Coverage:** {coverage:.2f}\n",
    "    \"\"\"\n",
    "\n",
    "# Example: Recommend a car for a specific user & travelCode\n",
    "user_id = 0\n",
    "travel_code = 39  # Replace with actual travel code from dataset\n",
    "top_n = 10  # Change this to Top-N recommendations\n",
    "top_cars = recommend_car(user_id, travel_code, top_n)\n",
    "print(f\"Recommended Cars for User {user_id} (Travel Code {travel_code}): {top_cars}\")\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(evaluate_recommendations(top_n=10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Cars for User 0 (Travel Code 22): [['Sedan' 'Sixt']\n",
      " ['Hatchback' 'Enterprise']\n",
      " ['Sedan' 'Sixt']\n",
      " ['Luxury' 'Enterprise']\n",
      " ['Sedan' 'Hertz']\n",
      " ['Sedan' 'Enterprise']\n",
      " ['Hatchback' 'Enterprise']\n",
      " ['Hatchback' 'Enterprise']\n",
      " ['Sedan' 'Hertz']\n",
      " ['SUV' 'Enterprise']]\n",
      "\n",
      "    ðŸ”¹ **Precision:** 0.10\n",
      "    ðŸ”¹ **Recall:** 1.00\n",
      "    ðŸ”¹ **Coverage:** 1.00\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load dataset\n",
    "file_path = \"D:/Make_my_trip/previous_ds/2.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Filter users who booked flights and hotels but NOT a car\n",
    "users_without_cars = df[(df['flight_price'].notna()) & \n",
    "                        (df['total_hotel'].notna()) & \n",
    "                        (df['carType'].isna())].copy()\n",
    "\n",
    "# Users who have booked a car (used for recommendations)\n",
    "users_with_cars = df[df['carType'].notna()].copy()\n",
    "\n",
    "# Fill missing values in feature columns\n",
    "feature_cols = ['pickupLocation', 'dropoffLocation', 'rentalAgency', 'fuelPolicy']\n",
    "# feature_cols = ['pickupLocation', 'dropoffLocation']\n",
    "\n",
    "users_with_cars[feature_cols] = users_with_cars[feature_cols].fillna(\"Unknown\")\n",
    "users_without_cars[feature_cols] = users_without_cars[feature_cols].fillna(\"Unknown\")\n",
    "\n",
    "# Add travelCode to the combined feature set\n",
    "users_with_cars['combined_features'] = users_with_cars[feature_cols + ['travelCode']].astype(str).agg(' '.join, axis=1)\n",
    "users_without_cars['combined_features'] = users_without_cars[feature_cols + ['travelCode']].astype(str).agg(' '.join, axis=1)\n",
    "\n",
    "# Reset index\n",
    "users_without_cars.reset_index(drop=True, inplace=True)\n",
    "users_with_cars.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# common_travel_codes = set(users_without_cars['travelCode']).intersection(set(users_with_cars['travelCode']))\n",
    "# print(\"Common Travel Codes:\", common_travel_codes)\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix_with_cars = tfidf.fit_transform(users_with_cars['combined_features'])\n",
    "tfidf_matrix_without_cars = tfidf.transform(users_without_cars['combined_features'])\n",
    "\n",
    "# Precompute cosine similarity between all users\n",
    "cosine_sim = cosine_similarity(tfidf_matrix_without_cars, tfidf_matrix_with_cars)\n",
    "\n",
    "# Group users_with_cars by travelCode for faster lookup\n",
    "cars_by_travel_code = users_with_cars.groupby('travelCode')\n",
    "\n",
    "# Convert DataFrame to NumPy arrays for faster lookups\n",
    "user_ids_without_cars = users_without_cars['user_id'].to_numpy()\n",
    "travel_codes_without_cars = users_without_cars['travelCode'].to_numpy()\n",
    "\n",
    "# Function to recommend a car for a specific user (Top-N) per travelCode\n",
    "def recommend_car(user_id, travel_code, top_n=3):\n",
    "    user_record = users_without_cars[users_without_cars['user_id'] == user_id]\n",
    "\n",
    "    if user_record.empty:\n",
    "        print(f\"âŒ No entry found for User {user_id}.\")\n",
    "        return \"No suitable recommendations found.\"\n",
    "\n",
    "    # Find similar users (ignoring exact travelCode match)\n",
    "    user_index = user_record.index[0]\n",
    "    top_indices = cosine_sim[user_index].argsort()[-top_n:][::-1]\n",
    "\n",
    "    recommended_cars = users_with_cars.iloc[top_indices][['carType', 'rentalAgency']].values\n",
    "\n",
    "    if recommended_cars.size == 0:\n",
    "        print(f\"âš ï¸ No cars found for similar users.\")\n",
    "    \n",
    "    return recommended_cars if recommended_cars.size > 0 else \"No suitable recommendations found.\"\n",
    "\n",
    "# Function to evaluate recommendations\n",
    "def evaluate_recommendations(top_n=3):\n",
    "    total_users = len(users_without_cars)\n",
    "    successful_recommendations = 0\n",
    "    total_recommendations = 0\n",
    "    unique_recommended_cars = set()\n",
    "    unique_actual_cars = set(users_with_cars['carType'].unique())\n",
    "\n",
    "    for user_index, (user_id, travel_code) in enumerate(zip(user_ids_without_cars, travel_codes_without_cars)):\n",
    "        top_cars = recommend_car(user_id, travel_code, top_n)\n",
    "\n",
    "        if isinstance(top_cars, str):  # Skip if no recommendations\n",
    "            continue\n",
    "\n",
    "        total_recommendations += len(top_cars)\n",
    "        unique_recommended_cars.update([car[0] for car in top_cars])\n",
    "\n",
    "        # Check if any recommended car matches the actual booked cars\n",
    "        if any(car[0] in unique_actual_cars for car in top_cars):\n",
    "            successful_recommendations += 1\n",
    "\n",
    "        # # Print progress every 1000 users\n",
    "        # if user_index % 1000 == 0:\n",
    "        #     print(f\"Processed {user_index}/{total_users} users...\")\n",
    "\n",
    "    # Compute evaluation metrics\n",
    "    precision = successful_recommendations / total_recommendations if total_recommendations > 0 else 0\n",
    "    recall = successful_recommendations / total_users if total_users > 0 else 0\n",
    "    coverage = len(unique_recommended_cars) / len(unique_actual_cars) if len(unique_actual_cars) > 0 else 0\n",
    "\n",
    "    return f\"\"\"\n",
    "    ðŸ”¹ **Precision:** {precision:.2f}\n",
    "    ðŸ”¹ **Recall:** {recall:.2f}\n",
    "    ðŸ”¹ **Coverage:** {coverage:.2f}\n",
    "    \"\"\"\n",
    "\n",
    "# Example: Recommend a car for a specific user & travelCode\n",
    "user_id = 0\n",
    "travel_code = 22  # Replace with actual travel code from dataset\n",
    "top_n = 10  # Change this to Top-N recommendations\n",
    "top_cars = recommend_car(user_id, travel_code, top_n)\n",
    "print(f\"Recommended Cars for User {user_id} (Travel Code {travel_code}): {top_cars}\")\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(evaluate_recommendations(top_n=10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Cars for User 1104 (Travel Code 111548): [['Hatchback' 'Avis']\n",
      " ['Luxury' 'Hertz']\n",
      " ['Hatchback' 'Enterprise']\n",
      " ['Sedan' 'Hertz']\n",
      " ['Hatchback' 'Enterprise']\n",
      " ['Luxury' 'Enterprise']\n",
      " ['Hatchback' 'Budget']\n",
      " ['Luxury' 'Sixt']\n",
      " ['Sedan' 'Avis']\n",
      " ['Luxury' 'Sixt']]\n",
      "\n",
      "    ðŸ”¹ **Precision:** 0.00\n",
      "    ðŸ”¹ **Recall:** 0.00\n",
      "    ðŸ”¹ **Coverage:** 0.75\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "file_path = \"D:/Make_my_trip/previous_ds/2.csv\"  # Replace with your file path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Data preprocessing (same as before)\n",
    "users_without_cars = df[(df['flight_price'].notna()) & \n",
    "                        (df['total_hotel'].notna()) & \n",
    "                        (df['carType'].isna())].copy()\n",
    "\n",
    "users_with_cars = df[df['carType'].notna()].copy()\n",
    "\n",
    "feature_cols = ['pickupLocation', 'dropoffLocation', 'rentalAgency', 'fuelPolicy']\n",
    "users_with_cars[feature_cols] = users_with_cars[feature_cols].fillna(\"Unknown\")\n",
    "users_without_cars[feature_cols] = users_without_cars[feature_cols].fillna(\"Unknown\")\n",
    "\n",
    "users_with_cars['combined_features'] = users_with_cars[feature_cols + ['travelCode']].astype(str).agg(' '.join, axis=1)\n",
    "users_without_cars['combined_features'] = users_without_cars[feature_cols + ['travelCode']].astype(str).agg(' '.join, axis=1)\n",
    "\n",
    "users_without_cars.reset_index(drop=True, inplace=True)\n",
    "users_with_cars.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_users_with_cars, test_users_with_cars = train_test_split(users_with_cars, test_size=0.2, random_state=42)\n",
    "train_users_without_cars, test_users_without_cars = train_test_split(users_without_cars, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- Improved TF-IDF Vectorization ---\n",
    "tfidf = TfidfVectorizer(stop_words='english', ngram_range=(1, 2), min_df=2)  # Tuned parameters\n",
    "tfidf_matrix_with_cars = tfidf.fit_transform(train_users_with_cars['combined_features'])\n",
    "tfidf_matrix_without_cars = tfidf.transform(test_users_without_cars['combined_features'])\n",
    "\n",
    "\n",
    "# Precompute cosine similarity between all users (on the training set)\n",
    "cosine_sim = cosine_similarity(tfidf_matrix_without_cars, tfidf_matrix_with_cars)\n",
    "\n",
    "# Recommendation function (FIXED)\n",
    "def recommend_car(user_id, travel_code, top_n=3):\n",
    "    user_record = test_users_without_cars[test_users_without_cars['user_id'] == user_id]\n",
    "\n",
    "    if user_record.empty:\n",
    "        print(f\"âŒ No entry found for User {user_id}.\")\n",
    "        return \"No suitable recommendations found.\"\n",
    "\n",
    "    user_index_tfidf = np.where(test_users_without_cars['user_id'] == user_id)[0][0]\n",
    "\n",
    "    top_indices = cosine_sim[user_index_tfidf].argsort()[-top_n:][::-1]\n",
    "\n",
    "    recommended_cars = train_users_with_cars.iloc[top_indices][['carType', 'rentalAgency']].values\n",
    "\n",
    "    if recommended_cars.size == 0:\n",
    "        print(f\"âš ï¸ No cars found for similar users.\")\n",
    "\n",
    "    return recommended_cars if recommended_cars.size > 0 else \"No suitable recommendations found.\"\n",
    "\n",
    "\n",
    "# Evaluation function (modified for train/test split)\n",
    "def evaluate_recommendations(top_n=3):\n",
    "    total_users = len(test_users_without_cars)\n",
    "    successful_recommendations = 0\n",
    "    total_recommendations = 0\n",
    "    unique_recommended_cars = set()\n",
    "    unique_actual_cars = set(train_users_with_cars['carType'].unique())\n",
    "\n",
    "    user_ids_without_cars = test_users_without_cars['user_id'].to_numpy()\n",
    "    travel_codes_without_cars = test_users_without_cars['travelCode'].to_numpy()\n",
    "\n",
    "    for user_index, (user_id, travel_code) in enumerate(zip(user_ids_without_cars, travel_codes_without_cars)):\n",
    "        top_cars = recommend_car(user_id, travel_code, top_n)\n",
    "\n",
    "        if isinstance(top_cars, str):\n",
    "            continue\n",
    "\n",
    "        total_recommendations += len(top_cars)\n",
    "        unique_recommended_cars.update([car[0] for car in top_cars])\n",
    "\n",
    "        actual_cars = train_users_with_cars[train_users_with_cars['travelCode'] == travel_code]['carType'].values\n",
    "        if len(actual_cars) > 0:\n",
    "            if any(car[0] in actual_cars for car in top_cars):\n",
    "                successful_recommendations += 1\n",
    "\n",
    "    precision = successful_recommendations / total_recommendations if total_recommendations > 0 else 0\n",
    "    recall = successful_recommendations / total_users if total_users > 0 else 0\n",
    "    coverage = len(unique_recommended_cars) / len(unique_actual_cars) if len(unique_actual_cars) > 0 else 0\n",
    "\n",
    "    return f\"\"\"\n",
    "    ðŸ”¹ **Precision:** {precision:.2f}\n",
    "    ðŸ”¹ **Recall:** {recall:.2f}\n",
    "    ðŸ”¹ **Coverage:** {coverage:.2f}\n",
    "    \"\"\"\n",
    "\n",
    "# Example and Evaluation (on the TEST set)\n",
    "example_user_id = test_users_without_cars['user_id'].iloc[0]\n",
    "example_travel_code = test_users_without_cars['travelCode'].iloc[0]\n",
    "top_n_example = 10\n",
    "example_recommendations = recommend_car(example_user_id, example_travel_code, top_n_example)\n",
    "print(f\"Recommended Cars for User {example_user_id} (Travel Code {example_travel_code}): {example_recommendations}\")\n",
    "\n",
    "\n",
    "print(evaluate_recommendations(top_n=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Separating the dataset into two csv files one with the users who have booked a car and the other with the user who did not book the car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Users successfully separated into two datasets!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the original dataset\n",
    "file_path = \"D:/Make_my_trip/previous_ds/2.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Separate users\n",
    "users_with_cars = df[df['carType'].notna()]  # Users who booked a car\n",
    "users_without_cars = df[df['carType'].isna()]  # Users who did NOT book a car\n",
    "\n",
    "# Save them as CSV files\n",
    "users_with_cars.to_csv(\"users_with_cars.csv\", index=False)\n",
    "users_without_cars.to_csv(\"users_without_cars.csv\", index=False)\n",
    "\n",
    "print(\"âœ… Users successfully separated into two datasets!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speed optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasketchNote: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Obtaining dependency information for datasketch from https://files.pythonhosted.org/packages/8d/24/c8b0570c17c64e9d00485ac6f325c3a7ba19ea8b3385c73c85a26a519d77/datasketch-1.6.5-py3-none-any.whl.metadata\n",
      "  Downloading datasketch-1.6.5-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: numpy>=1.11 in d:\\make_my_trip\\recommendation\\recomm\\lib\\site-packages (from datasketch) (2.2.2)\n",
      "Requirement already satisfied: scipy>=1.0.0 in d:\\make_my_trip\\recommendation\\recomm\\lib\\site-packages (from datasketch) (1.15.1)\n",
      "Downloading datasketch-1.6.5-py3-none-any.whl (89 kB)\n",
      "   ---------------------------------------- 0.0/89.2 kB ? eta -:--:--\n",
      "   ---- ----------------------------------- 10.2/89.2 kB ? eta -:--:--\n",
      "   --------- ------------------------------ 20.5/89.2 kB 165.2 kB/s eta 0:00:01\n",
      "   --------- ------------------------------ 20.5/89.2 kB 165.2 kB/s eta 0:00:01\n",
      "   --------- ------------------------------ 20.5/89.2 kB 165.2 kB/s eta 0:00:01\n",
      "   -------------------------------- ------- 71.7/89.2 kB 302.7 kB/s eta 0:00:01\n",
      "   -------------------------------- ------- 71.7/89.2 kB 302.7 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 89.2/89.2 kB 266.0 kB/s eta 0:00:00\n",
      "Installing collected packages: datasketch\n",
      "Successfully installed datasketch-1.6.5\n"
     ]
    }
   ],
   "source": [
    "#pip install datasketch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hetvi\\AppData\\Local\\Temp\\ipykernel_16436\\4224198477.py:17: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Unknown' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  users_without_cars.fillna(\"Unknown\", inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ Recommended Cars for Users:\n",
      "User 0: ['Hatchback', 'Sedan', 'Luxury']\n",
      "User 1: ['Luxury', 'Sedan', 'SUV']\n",
      "User 2: ['Luxury', 'SUV', 'Sedan']\n",
      "User 3: ['SUV', 'Sedan', 'Luxury']\n",
      "User 4: ['Luxury', 'SUV', 'Sedan']\n",
      "User 5: ['Hatchback', 'Sedan', 'SUV']\n",
      "User 6: ['Luxury', 'Sedan', 'SUV']\n",
      "User 7: ['Luxury', 'SUV', 'Sedan']\n",
      "User 8: ['SUV', 'Luxury', 'Sedan']\n",
      "User 9: ['Luxury', 'Sedan', 'SUV']\n",
      "\n",
      "ðŸ“Š **Evaluation Metrics:**\n",
      "ðŸ”¹ **Precision:** 0.24\n",
      "ðŸ”¹ **Recall:** 0.24\n",
      "ðŸ”¹ **Coverage:** 1.00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "# Load datasets efficiently\n",
    "users_with_cars = pd.read_csv(\"users_with_cars.csv\", low_memory=False)\n",
    "users_without_cars = pd.read_csv(\"users_without_cars.csv\", low_memory=False)\n",
    "\n",
    "# Fill missing values\n",
    "users_with_cars.fillna(\"Unknown\", inplace=True)\n",
    "users_without_cars.fillna(\"Unknown\", inplace=True)\n",
    "\n",
    "# Feature Engineering\n",
    "def create_combined_features(df):\n",
    "    return (\n",
    "        df[\"pickupLocation\"].astype(str) + \" \" +\n",
    "        df[\"dropoffLocation\"].astype(str) + \" \" +\n",
    "        df[\"rentalAgency\"].astype(str) + \" \" +\n",
    "        df[\"flightType\"].astype(str) + \" \" +\n",
    "        df[\"carType\"].astype(str) + \" \" +\n",
    "        df[\"travelCode\"].astype(str)\n",
    "    )\n",
    "\n",
    "users_with_cars[\"combined_features\"] = create_combined_features(users_with_cars)\n",
    "users_without_cars[\"combined_features\"] = create_combined_features(users_without_cars)\n",
    "\n",
    "# âœ… **Fast Hashing Vectorizer Instead of TF-IDF**\n",
    "vectorizer = HashingVectorizer(n_features=500, alternate_sign=False)\n",
    "hash_matrix_with_cars = vectorizer.fit_transform(users_with_cars[\"combined_features\"])\n",
    "hash_matrix_without_cars = vectorizer.transform(users_without_cars[\"combined_features\"])\n",
    "\n",
    "# âœ… **Efficient Cosine Similarity Calculation**\n",
    "cosine_sim = cosine_similarity(hash_matrix_without_cars, hash_matrix_with_cars)\n",
    "\n",
    "# âœ… **MinHash for Fast Jaccard Similarity**\n",
    "def get_minhash(text, num_perm=128):\n",
    "    \"\"\"Generate MinHash for a given text\"\"\"\n",
    "    m = MinHash(num_perm=num_perm)\n",
    "    for word in text.split():\n",
    "        m.update(word.encode(\"utf8\"))\n",
    "    return m\n",
    "\n",
    "# Build MinHash LSH for fast similarity search\n",
    "lsh = MinHashLSH(threshold=0.5, num_perm=128)\n",
    "minhash_dict = {}\n",
    "\n",
    "for i, text in enumerate(users_with_cars[\"combined_features\"]):\n",
    "    mh = get_minhash(text)\n",
    "    lsh.insert(i, mh)\n",
    "    minhash_dict[i] = mh\n",
    "\n",
    "# Calculate Jaccard Similarity using MinHash\n",
    "jaccard_sim = np.zeros((len(users_without_cars), len(users_with_cars)))\n",
    "\n",
    "for i, text in enumerate(users_without_cars[\"combined_features\"]):\n",
    "    mh = get_minhash(text)\n",
    "    similar_indices = lsh.query(mh)  # Get similar users fast\n",
    "    for j in similar_indices:\n",
    "        jaccard_sim[i, j] = mh.jaccard(minhash_dict[j])\n",
    "\n",
    "# âœ… **Hybrid Similarity Calculation**\n",
    "final_similarity = (0.9 * cosine_sim) + (0.1 * jaccard_sim)\n",
    "\n",
    "# âœ… **Optimized Recommendation Extraction (NumPy argpartition)**\n",
    "recommendations = {}\n",
    "top_k = 3\n",
    "\n",
    "for i, user_id in enumerate(users_without_cars[\"user_id\"]):\n",
    "    top_indices = np.argpartition(final_similarity[i], -top_k)[-top_k:]\n",
    "    top_indices = top_indices[np.argsort(final_similarity[i][top_indices])[::-1]]\n",
    "    recommended_cars = users_with_cars.iloc[top_indices][\"carType\"].tolist()\n",
    "    recommendations[user_id] = recommended_cars if recommended_cars else [\"No suitable recommendations.\"]\n",
    "\n",
    "# âœ… **Fast Decision Tree Instead of RandomForest**\n",
    "label_encoder = LabelEncoder()\n",
    "users_with_cars[\"carType_encoded\"] = label_encoder.fit_transform(users_with_cars[\"carType\"].astype(str))\n",
    "\n",
    "X = users_with_cars[[\"travelCode\", \"rentalDuration\", \"totalDistance\"]].astype(float)\n",
    "y = users_with_cars[\"carType_encoded\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = DecisionTreeClassifier(max_depth=5, random_state=42)  # Shallow tree for fast prediction\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# âœ… **Evaluation**\n",
    "def evaluate_recommendations():\n",
    "    y_pred = model.predict(X_test)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=1)\n",
    "    recall = recall_score(y_test, y_pred, average='weighted', zero_division=1)\n",
    "    coverage = len(set(y_pred)) / max(len(set(y_test)), 1)\n",
    "    return precision, recall, coverage\n",
    "\n",
    "# âœ… **Display Results (Limit to 10 Users for Speed)**\n",
    "print(\"\\nðŸ”¹ Recommended Cars for Users:\")\n",
    "for user_id, cars in list(recommendations.items())[:10]:\n",
    "    print(f\"User {user_id}: {cars}\")\n",
    "\n",
    "precision, recall, coverage = evaluate_recommendations()\n",
    "\n",
    "print(\"\\nðŸ“Š **Evaluation Metrics:**\")\n",
    "print(f\"ðŸ”¹ **Precision:** {precision:.2f}\")\n",
    "print(f\"ðŸ”¹ **Recall:** {recall:.2f}\")\n",
    "print(f\"ðŸ”¹ **Coverage:** {coverage:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "following are the changes that are done in the new code below:\n",
    "\n",
    "âœ… Feature Engineering Improvements\n",
    "\n",
    "âœ… Weighted Hybrid Similarity (using optimal weights)\n",
    "\n",
    "âœ… One-Hot Encoding for categorical features\n",
    "\n",
    "âœ… Using XGBoost for better predictions\n",
    "\n",
    "âœ… Efficient Filtering of Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hetvi\\AppData\\Local\\Temp\\ipykernel_6456\\2703936123.py:12: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Unknown' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.fillna(\"Unknown\", inplace=True)  # Ensure missing values are handled correctly\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Cars for User 0: ['Sedan', 'Sedan', 'Sedan']\n",
      "\n",
      "Evaluation Metrics:\n",
      "ðŸ”¹ Precision: 0.23\n",
      "ðŸ”¹ Recall: 0.24\n",
      "ðŸ”¹ Coverage: 1.00\n"
     ]
    }
   ],
   "source": [
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "    def preprocess_data(file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        df.fillna(\"Unknown\", inplace=True)  # Ensure missing values are handled correctly\n",
    "        return df\n",
    "\n",
    "    def create_combined_features(df):\n",
    "        return (\n",
    "            df[\"pickupLocation\"].astype(str) + \" \" +\n",
    "            df[\"dropoffLocation\"].astype(str) + \" \" +\n",
    "            df[\"rentalAgency\"].astype(str) + \" \" +\n",
    "            df[\"flightType\"].astype(str) + \" \" +\n",
    "            df[\"carType\"].astype(str) + \" \" +\n",
    "            df[\"travelCode\"].astype(str)\n",
    "        )\n",
    "\n",
    "    def recommend_car_for_user(target_user_id, users_with_cars, users_without_cars):\n",
    "        users_with_cars[\"combined_features\"] = create_combined_features(users_with_cars)\n",
    "        users_without_cars[\"combined_features\"] = create_combined_features(users_without_cars)\n",
    "        \n",
    "        tfidf = TfidfVectorizer(max_features=500, stop_words='english')\n",
    "        tfidf_matrix_with_cars = tfidf.fit_transform(users_with_cars[\"combined_features\"])\n",
    "        tfidf_matrix_without_cars = tfidf.transform(users_without_cars[\"combined_features\"])\n",
    "        \n",
    "        cosine_sim = cosine_similarity(tfidf_matrix_without_cars, tfidf_matrix_with_cars)\n",
    "        \n",
    "        target_index = users_without_cars[users_without_cars[\"user_id\"] == target_user_id].index[0]\n",
    "        \n",
    "        top_indices = np.argsort(cosine_sim[target_index])[::-1][:3]  # Top-3 recommendations\n",
    "        recommended_cars = users_with_cars.iloc[top_indices][\"carType\"].tolist()\n",
    "        \n",
    "        return recommended_cars if recommended_cars else \"No suitable recommendations found.\"\n",
    "\n",
    "    def train_ml_model(users_with_cars):\n",
    "        label_encoder = LabelEncoder()\n",
    "        users_with_cars[\"carType_encoded\"] = label_encoder.fit_transform(users_with_cars[\"carType\"].astype(str))\n",
    "        \n",
    "        X = users_with_cars[[\"travelCode\", \"rentalDuration\", \"totalDistance\"]]\n",
    "        y = users_with_cars[\"carType_encoded\"]\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        model = RandomForestClassifier(n_estimators=50, max_depth=5, random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        y_pred = model.predict(X_test)\n",
    "        precision = precision_score(y_test, y_pred, average='weighted')\n",
    "        recall = recall_score(y_test, y_pred, average='weighted')\n",
    "        coverage = len(set(y_pred)) / len(set(y_test))\n",
    "        \n",
    "        return precision, recall, coverage\n",
    "\n",
    "    # Load data\n",
    "    users_with_cars = preprocess_data(\"users_with_cars.csv\")\n",
    "    users_without_cars = preprocess_data(\"users_without_cars.csv\")\n",
    "\n",
    "    # Choose a user to recommend a car for\n",
    "    target_user_id = 0  # Change this to any specific user_id\n",
    "    recommended_cars = recommend_car_for_user(target_user_id, users_with_cars, users_without_cars)\n",
    "\n",
    "    # Train ML model and evaluate\n",
    "    precision, recall, coverage = train_ml_model(users_with_cars)\n",
    "\n",
    "    # Display results\n",
    "    print(f\"Recommended Cars for User {target_user_id}: {recommended_cars}\")\n",
    "    print(\"\\nEvaluation Metrics:\")\n",
    "    print(f\"ðŸ”¹ Precision: {precision:.2f}\")\n",
    "    print(f\"ðŸ”¹ Recall: {recall:.2f}\")\n",
    "    print(f\"ðŸ”¹ Coverage: {coverage:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# new "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'implicit'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m coo_matrix\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mimplicit\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mals\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AlternatingLeastSquares\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m precision_score, recall_score\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'implicit'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.sparse import coo_matrix\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "# Load dataset\n",
    "file_path = \"D:/Make_my_trip/previous_ds/2.csv\"  # Replace with your path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Data Preprocessing\n",
    "# Ensure the required columns are present: 'user_id' and 'carType' \n",
    "df = df.dropna(subset=['user_id', 'carType'])\n",
    "\n",
    "# Map carType to category codes for efficient processing\n",
    "df['carType_id'] = df['carType'].astype('category').cat.codes\n",
    "\n",
    "# Create user-item interaction matrix\n",
    "interaction_data = df[['user_id', 'carType_id']].copy()\n",
    "interaction_data['interaction'] = 1  # Implicit interaction feedback\n",
    "\n",
    "user_item_matrix = coo_matrix((\n",
    "    interaction_data['interaction'], \n",
    "    (interaction_data['user_id'], interaction_data['carType_id'])\n",
    "))\n",
    "\n",
    "# Initialize and Train ALS Model\n",
    "model = AlternatingLeastSquares(factors=50, regularization=0.1, iterations=30)\n",
    "model.fit(user_item_matrix)\n",
    "\n",
    "# Recommendation for a user\n",
    "def recommend_cars(user_id, top_n=5):\n",
    "    user_interaction = user_item_matrix.getrow(user_id)\n",
    "    recommended = model.recommend(user_id, user_interaction, N=top_n)\n",
    "    \n",
    "    car_id_mapping = df[['carType', 'carType_id']].drop_duplicates()\n",
    "    car_id_to_name = car_id_mapping.set_index('carType_id')['carType'].to_dict()\n",
    "    \n",
    "    recommended_cars = [car_id_to_name[car_id] for car_id, _ in recommended]\n",
    "    return recommended_cars\n",
    "\n",
    "# Example usage\n",
    "example_user_id = df['user_id'].iloc[0]  # Replace with a specific user ID if available\n",
    "print(f\"Recommended cars for user {example_user_id}: {recommend_cars(example_user_id)}\")\n",
    "\n",
    "# Evaluation\n",
    "def evaluate_als_model(test_data, top_n=5):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    for user_id in test_data['user_id'].unique():\n",
    "        actual_car_types = test_data[test_data['user_id'] == user_id]['carType_id'].values\n",
    "        predicted_cars = recommend_cars(user_id, top_n)\n",
    "        \n",
    "        if not predicted_cars or len(actual_car_types) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Map recommended car names to IDs\n",
    "        predicted_car_ids = [df[df['carType'] == car]['carType_id'].values[0] for car in predicted_cars if car in df['carType'].unique()]\n",
    "        \n",
    "        # Update ground truth and predictions\n",
    "        y_true.append(list(actual_car_types))\n",
    "        y_pred.append(predicted_car_ids)\n",
    "    \n",
    "    # Flatten lists for precision and recall\n",
    "    y_true_flat = [item for sublist in y_true for item in sublist]\n",
    "    y_pred_flat = [item for sublist in y_pred for item in sublist]\n",
    "    \n",
    "    # Compute evaluation metrics\n",
    "    precision = precision_score(y_true_flat, y_pred_flat, average='macro')\n",
    "    recall = recall_score(y_true_flat, y_pred_flat, average='macro')\n",
    "    \n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "\n",
    "# Split test data\n",
    "test_users_with_cars = df.sample(frac=0.2, random_state=42)\n",
    "evaluate_als_model(test_users_with_cars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
