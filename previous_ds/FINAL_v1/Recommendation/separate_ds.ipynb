{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   travelCode  User_ID_x      Departure             Arrival  flightType  \\\n",
      "0           0          0    Recife (PE)  Florianopolis (SC)  firstClass   \n",
      "1           2          0   Aracaju (SE)       Salvador (BH)  firstClass   \n",
      "2           7          0   Aracaju (SE)       Salvador (BH)    economic   \n",
      "3          11          0  Brasilia (DF)       Salvador (BH)     premium   \n",
      "4          13          0    Recife (PE)  Florianopolis (SC)  firstClass   \n",
      "\n",
      "   Flight_price  Flight_duration  Flight_Distance Flight_agency  \\\n",
      "0       1434.38             1.76           676.53   FlyingDrops   \n",
      "1       1684.05             2.16           830.86       CloudFy   \n",
      "2        964.83             2.16           830.86       CloudFy   \n",
      "3       1268.97             1.76           676.56       Rainbow   \n",
      "4       1434.38             1.76           676.53   FlyingDrops   \n",
      "\n",
      "  Departure_date  User_ID_y Hotel_Name       Arrival_place  Hotel_stay  \\\n",
      "0     26/09/2019          0    Hotel A  Florianopolis (SC)           4   \n",
      "1     10/10/2019          0    Hotel K       Salvador (BH)           2   \n",
      "2     14/11/2019          0    Hotel K       Salvador (BH)           3   \n",
      "3     12/12/2019          0    Hotel K       Salvador (BH)           4   \n",
      "4     26/12/2019          0    Hotel A  Florianopolis (SC)           1   \n",
      "\n",
      "   Hotel_per_day_price    Check-in  Hotel_TotalPrice  \n",
      "0               313.02  09/26/2019           1252.08  \n",
      "1               263.41  10/10/2019            526.82  \n",
      "2               263.41  11/14/2019            790.23  \n",
      "3               263.41  12/12/2019           1053.64  \n",
      "4               313.02  12/26/2019            313.02  \n"
     ]
    }
   ],
   "source": [
    "# merging the flight and hotel dataset:\n",
    "\n",
    "# Load the datasets\n",
    "flights = pd.read_excel(r\"D:\\Make_my_trip\\FinalDataset\\FlightFINALdataset.xlsx\")\n",
    "hotels = pd.read_excel(r\"D:\\Make_my_trip\\FinalDataset\\HotelFINALdataset.xlsx\")\n",
    "\n",
    "# Merge on a common column (e.g., 'user_id' or 'travelCode')\n",
    "merged_flights_hotels = flights.merge(hotels, on=\"travelCode\", how=\"inner\")\n",
    "\n",
    "# Save the merged dataset\n",
    "merged_flights_hotels.to_csv(\"flights_hotels.csv\", index=False)\n",
    "\n",
    "# Display the first few rows\n",
    "print(merged_flights_hotels.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car_booking\n",
      "0    36779\n",
      "1     3773\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load datasets\n",
    "flights_hotels = pd.read_csv(\"flights_hotels.csv\")\n",
    "cars = pd.read_excel(r\"D:\\Make_my_trip\\FinalDataset\\CarFINALdataset.xlsx\")\n",
    "\n",
    "# Merge datasets on user_id or travelCode\n",
    "merged_data = flights_hotels.merge(cars, on=\"travelCode\", how=\"left\", indicator=True)\n",
    "\n",
    "# Label users who booked a car (1) and those who didn't (0)\n",
    "merged_data[\"car_booking\"] = (merged_data[\"_merge\"] == \"both\").astype(int)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "merged_data.drop(columns=[\"_merge\"], inplace=True)\n",
    "\n",
    "# Check class balance\n",
    "print(merged_data[\"car_booking\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the training and recommendation sets:\n",
    "\n",
    "Training set:\n",
    "\n",
    "~ contains the users who have booked a car \n",
    "\n",
    "~ plus an equal number of randomly selected users who did not book a car.\n",
    "\n",
    "~ this will balance the dataset and prevent the model bias.\n",
    "\n",
    "Recommendation Set:\n",
    "\n",
    "~ contains the remaining users who booked flight and hotel but not a car.\n",
    "\n",
    "~ the model will predict if they are likley to book a car.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Shape: (7546, 29)\n",
      "Recommendation Set Shape: (33006, 29)\n"
     ]
    }
   ],
   "source": [
    "# Load the merged flights-hotels-car dataset\n",
    "merged_data = pd.read_csv(\"flights_hotels.csv\")\n",
    "\n",
    "# Load car booking data (contains users who booked a car)\n",
    "cars = pd.read_excel(r\"D:\\Make_my_trip\\FinalDataset\\CarFINALdataset.xlsx\")\n",
    "\n",
    "# Merge to identify users who booked cars\n",
    "merged_data = merged_data.merge(cars, on=\"travelCode\", how=\"left\", indicator=True)\n",
    "\n",
    "# Label users who booked a car (1) and those who didn't (0)\n",
    "merged_data[\"car_booking\"] = (merged_data[\"_merge\"] == \"both\").astype(int)\n",
    "merged_data.drop(columns=[\"_merge\"], inplace=True)\n",
    "\n",
    "# Split into car bookers (1) and non-car bookers (0)\n",
    "booked_cars = merged_data[merged_data[\"car_booking\"] == 1]\n",
    "not_booked_cars = merged_data[merged_data[\"car_booking\"] == 0]\n",
    "\n",
    "# Select equal number of non-car bookings (to balance training set)\n",
    "not_booked_sample = not_booked_cars.sample(n=len(booked_cars), random_state=42)\n",
    "\n",
    "# Training Data: Equal number of users who booked and didn't book a car\n",
    "train_data = pd.concat([booked_cars, not_booked_sample])\n",
    "\n",
    "# Recommendation Data: Remaining users who did not book a car\n",
    "recommendation_data = not_booked_cars.drop(not_booked_sample.index)\n",
    "\n",
    "# Save datasets\n",
    "train_data.to_csv(\"train_data.csv\", index=False)\n",
    "recommendation_data.to_csv(\"recommendation_data.csv\", index=False)\n",
    "\n",
    "# Check dataset shapes\n",
    "print(\"Training Set Shape:\", train_data.shape)\n",
    "print(\"Recommendation Set Shape:\", recommendation_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['travelCode', 'User_ID_x', 'Departure', 'Arrival', 'flightType',\n",
       "       'Flight_price', 'Flight_duration', 'Flight_Distance', 'Flight_agency',\n",
       "       'Departure_date', 'User_ID_y', 'Hotel_Name', 'Arrival_place',\n",
       "       'Hotel_stay', 'Hotel_per_day_price', 'Check-in_x', 'Hotel_TotalPrice',\n",
       "       'User_ID', 'Check-in_y', 'pickupLocation', 'dropoffLocation', 'carType',\n",
       "       'rentalAgency', 'rentalDuration', 'Car_total_distance', 'fuelPolicy',\n",
       "       'Car_bookingStatus', 'total_rent_price', 'car_booking'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data= train_data.drop(columns=['User_ID_y', 'Check-in_y'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendation_data= recommendation_data.drop(columns=['User_ID_y', 'Check-in_y'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=train_data.drop(columns=['User_ID'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendation_data=recommendation_data.drop(columns=['User_ID'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['travelCode', 'User_ID_x', 'Departure', 'Arrival', 'flightType',\n",
       "       'Flight_price', 'Flight_duration', 'Flight_Distance', 'Flight_agency',\n",
       "       'Departure_date', 'Hotel_Name', 'Arrival_place', 'Hotel_stay',\n",
       "       'Hotel_per_day_price', 'Check-in_x', 'Hotel_TotalPrice',\n",
       "       'pickupLocation', 'dropoffLocation', 'carType', 'rentalAgency',\n",
       "       'rentalDuration', 'Car_total_distance', 'fuelPolicy',\n",
       "       'Car_bookingStatus', 'total_rent_price', 'car_booking'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['travelCode', 'User_ID_x', 'Departure', 'Arrival', 'flightType',\n",
       "       'Flight_price', 'Flight_duration', 'Flight_Distance', 'Flight_agency',\n",
       "       'Departure_date', 'Hotel_Name', 'Arrival_place', 'Hotel_stay',\n",
       "       'Hotel_per_day_price', 'Check-in_x', 'Hotel_TotalPrice',\n",
       "       'pickupLocation', 'dropoffLocation', 'carType', 'rentalAgency',\n",
       "       'rentalDuration', 'Car_total_distance', 'fuelPolicy',\n",
       "       'Car_bookingStatus', 'total_rent_price', 'car_booking'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommendation_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.rename(columns={'User_ID_x':'User_ID','Check-in_x':'Check_in_Hotel'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendation_data.rename(columns={'User_ID_x':'User_ID','Check-in_x':'Check_in_Hotel'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['travelCode',\n",
       " 'User_ID',\n",
       " 'Departure',\n",
       " 'Arrival',\n",
       " 'flightType',\n",
       " 'Flight_price',\n",
       " 'Flight_duration',\n",
       " 'Flight_Distance',\n",
       " 'Flight_agency',\n",
       " 'Departure_date',\n",
       " 'Hotel_Name',\n",
       " 'Arrival_place',\n",
       " 'Hotel_stay',\n",
       " 'Hotel_per_day_price',\n",
       " 'Check_in_Hotel',\n",
       " 'Hotel_TotalPrice',\n",
       " 'pickupLocation',\n",
       " 'dropoffLocation',\n",
       " 'carType',\n",
       " 'rentalAgency',\n",
       " 'rentalDuration',\n",
       " 'Car_total_distance',\n",
       " 'fuelPolicy',\n",
       " 'Car_bookingStatus',\n",
       " 'total_rent_price',\n",
       " 'car_booking']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data for the likelihood prediction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# First, separate users who booked cars vs those who didn't\n",
    "car_bookers = train_data[train_data['car_booking'] == 1]\n",
    "non_car_bookers = train_data[train_data['car_booking'] == 0]\n",
    "\n",
    "# Randomly select 3773 users who booked cars\n",
    "if len(car_bookers) >3772:\n",
    "    car_bookers = car_bookers.sample(n=3773, random_state=42)\n",
    "else:\n",
    "    print(f\"Warning: Only {len(car_bookers)} users who booked cars available\")\n",
    "\n",
    "# Randomly select 3773 users who didn't book cars\n",
    "non_car_bookers_sample = non_car_bookers.sample(n=3773, random_state=42)\n",
    "\n",
    "# Create balanced training dataset\n",
    "training_data = pd.concat([car_bookers, non_car_bookers_sample])\n",
    "\n",
    "# The rest of non-car bookers become your recommendation set\n",
    "recommendation_set = non_car_bookers.drop(non_car_bookers_sample.index)\n",
    "\n",
    "# Drop irrelevant columns as identified earlier\n",
    "columns_to_drop = [\n",
    "    'travelCode',\n",
    "    'Departure',\n",
    "    'Arrival',\n",
    "    'flightType',\n",
    "    'Hotel_Name',\n",
    "    'Check_in_Hotel',\n",
    "    'pickupLocation',\n",
    "    'dropoffLocation',\n",
    "    'carType',\n",
    "    'rentalAgency',\n",
    "    'rentalDuration',\n",
    "    'Car_total_distance',\n",
    "    'fuelPolicy',\n",
    "    'Car_bookingStatus',\n",
    "    'total_rent_price'\n",
    "]\n",
    "\n",
    "training_data_cleaned = train_data.drop(columns=columns_to_drop)\n",
    "recommendation_set_cleaned = recommendation_data.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature preparation and splitting training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target for training\n",
    "X = training_data_cleaned.drop(columns=['car_booking', 'User_ID'])  # Remove target and ID\n",
    "y = training_data_cleaned['car_booking']\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "numerical_cols = X_train.select_dtypes(include=['float64', 'int64']).columns\n",
    "X_train[numerical_cols] = scaler.fit_transform(X_train[numerical_cols])\n",
    "X_test[numerical_cols] = scaler.transform(X_test[numerical_cols])\n",
    "\n",
    "# Handle categorical features if any (you may need to one-hot encode them)\n",
    "# For example, Flight_agency might be categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the likelihood prediction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns that need encoding: ['Flight_agency', 'Departure_date', 'Arrival_place']\n",
      "[[391 364]\n",
      " [351 404]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.52      0.52       755\n",
      "           1       0.53      0.54      0.53       755\n",
      "\n",
      "    accuracy                           0.53      1510\n",
      "   macro avg       0.53      0.53      0.53      1510\n",
      "weighted avg       0.53      0.53      0.53      1510\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First, let's identify categorical columns in your dataset\n",
    "categorical_cols = X_train.select_dtypes(include=['object']).columns\n",
    "print(f\"Categorical columns that need encoding: {list(categorical_cols)}\")\n",
    "\n",
    "# Now, let's use one-hot encoding for categorical variables\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Create a pipeline with preprocessing and the model\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Fit the pipeline on the raw data (without previous preprocessing)\n",
    "X_train_raw = training_data_cleaned.drop(columns=['car_booking', 'User_ID'])\n",
    "y_train_raw = training_data_cleaned['car_booking']\n",
    "X_test_raw = X_test.copy() if 'X_test' in locals() else None\n",
    "\n",
    "# Fit the pipeline\n",
    "pipeline.fit(X_train_raw, y_train_raw)\n",
    "\n",
    "# If you have a test set\n",
    "if X_test_raw is not None:\n",
    "    y_pred = pipeline.predict(X_test_raw)\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Improve the model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'classifier__max_depth': None, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__n_estimators': 300}\n",
      "Best cross-validation score: 0.2968\n",
      "[[392 363]\n",
      " [377 378]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.52      0.51       755\n",
      "           1       0.51      0.50      0.51       755\n",
      "\n",
      "    accuracy                           0.51      1510\n",
      "   macro avg       0.51      0.51      0.51      1510\n",
      "weighted avg       0.51      0.51      0.51      1510\n",
      "\n",
      "Top 10 most important features:\n",
      "                      Feature  Importance\n",
      "0                Flight_price    0.100699\n",
      "5            Hotel_TotalPrice    0.048846\n",
      "2             Flight_Distance    0.042694\n",
      "1             Flight_duration    0.041362\n",
      "3                  Hotel_stay    0.034695\n",
      "4         Hotel_per_day_price    0.016698\n",
      "6       Flight_agency_CloudFy    0.015199\n",
      "8       Flight_agency_Rainbow    0.014994\n",
      "7   Flight_agency_FlyingDrops    0.010118\n",
      "13  Departure_date_01/09/2020    0.006906\n"
     ]
    }
   ],
   "source": [
    "# 1. Try tuning the hyperparameters\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [100, 200, 300],\n",
    "    'classifier__max_depth': [None, 10, 20, 30],\n",
    "    'classifier__min_samples_split': [2, 5, 10],\n",
    "    'classifier__min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_raw, y_train_raw)\n",
    "\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Use the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test_raw)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# 2. Feature importance\n",
    "# Get feature names after one-hot encoding\n",
    "feature_names = (\n",
    "    numerical_cols.tolist() +\n",
    "    best_model.named_steps['preprocessor'].transformers_[1][1].get_feature_names_out(categorical_cols).tolist()\n",
    ")\n",
    "\n",
    "# Get importances\n",
    "importances = best_model.named_steps['classifier'].feature_importances_\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"Top 10 most important features:\")\n",
    "print(feature_importance.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users with car booking likelihood >= 0.7: 3902\n",
      "       User_ID  car_booking_likelihood\n",
      "39454     1303                0.976667\n",
      "4694       155                0.976667\n",
      "6457       209                0.976667\n",
      "5038       166                0.966667\n",
      "9483       300                0.966667\n"
     ]
    }
   ],
   "source": [
    "# Prepare the recommendation set (same preprocessing as training data)\n",
    "X_recommendation = recommendation_set_cleaned.drop(columns=['car_booking'])\n",
    "user_ids = recommendation_set_cleaned['User_ID']\n",
    "\n",
    "# Make predictions\n",
    "prediction_proba = best_model.predict_proba(X_recommendation)\n",
    "car_booking_likelihood = prediction_proba[:, 1]  # Probability of class 1 (booking a car)\n",
    "\n",
    "# Create recommendation dataframe\n",
    "recommendations = pd.DataFrame({\n",
    "    'User_ID': user_ids,\n",
    "    'car_booking_likelihood': car_booking_likelihood\n",
    "})\n",
    "\n",
    "# Sort by likelihood (highest first)\n",
    "recommendations = recommendations.sort_values('car_booking_likelihood', ascending=False)\n",
    "\n",
    "# Select users above a certain threshold (e.g., 0.7) for car recommendations\n",
    "threshold = 0.7\n",
    "likely_car_bookers = recommendations[recommendations['car_booking_likelihood'] >= threshold]\n",
    "\n",
    "print(f\"Number of users with car booking likelihood >= {threshold}: {len(likely_car_bookers)}\")\n",
    "print(likely_car_bookers.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data for car type recommendation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of car types booked:\n",
      "carType\n",
      "Sedan        990\n",
      "Hatchback    939\n",
      "Luxury       925\n",
      "SUV          919\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# First, get historical data of users who actually booked cars\n",
    "car_bookers_data = merged_data[merged_data['car_booking'] == 1].copy()\n",
    "\n",
    "# Check what car types are available\n",
    "car_types = car_bookers_data['carType'].value_counts()\n",
    "print(\"Distribution of car types booked:\")\n",
    "print(car_types)\n",
    "\n",
    "# Features that might influence car type choice\n",
    "car_type_features = [\n",
    "    'Flight_Distance', \n",
    "    'Flight_duration',\n",
    "    'Hotel_stay',\n",
    "    'Hotel_per_day_price',  # Indicator of budget/luxury preference\n",
    "    'Arrival_place',        # Different locations might require different cars\n",
    "    'Flight_price'          # Another budget indicator\n",
    "]\n",
    "\n",
    "# Prepare features and target for car type prediction\n",
    "X_car_type = car_bookers_data[car_type_features]\n",
    "y_car_type = car_bookers_data['carType']\n",
    "\n",
    "# Handle categorical features (like Arrival_place)\n",
    "categorical_cols_car = X_car_type.select_dtypes(include=['object']).columns\n",
    "numerical_cols_car = X_car_type.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# Create preprocessor for car type model\n",
    "car_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_cols_car),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols_car)\n",
    "    ])\n",
    "\n",
    "# Split the data\n",
    "X_train_car, X_test_car, y_train_car, y_test_car = train_test_split(\n",
    "    X_car_type, y_car_type, test_size=0.2, random_state=42, stratify=y_car_type\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the car type recommendation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Car Type Recommendation Model Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Hatchback       0.21      0.22      0.22       188\n",
      "      Luxury       0.24      0.25      0.25       185\n",
      "         SUV       0.22      0.21      0.22       184\n",
      "       Sedan       0.28      0.28      0.28       198\n",
      "\n",
      "    accuracy                           0.24       755\n",
      "   macro avg       0.24      0.24      0.24       755\n",
      "weighted avg       0.24      0.24      0.24       755\n",
      "\n",
      "\n",
      "Top factors influencing car type choice:\n",
      "                              Feature  Importance\n",
      "4                        Flight_price    0.572974\n",
      "2                          Hotel_stay    0.232146\n",
      "0                     Flight_Distance    0.073098\n",
      "1                     Flight_duration    0.069688\n",
      "3                 Hotel_per_day_price    0.016633\n",
      "5          Arrival_place_Aracaju (SE)    0.004566\n",
      "9            Arrival_place_Natal (RN)    0.004494\n",
      "11  Arrival_place_Rio de Janeiro (RJ)    0.004481\n",
      "10          Arrival_place_Recife (PE)    0.004019\n",
      "6         Arrival_place_Brasilia (DF)    0.003966\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create pipeline\n",
    "car_type_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', car_preprocessor),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Train model\n",
    "car_type_pipeline.fit(X_train_car, y_train_car)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_car = car_type_pipeline.predict(X_test_car)\n",
    "print(\"\\nCar Type Recommendation Model Performance:\")\n",
    "print(classification_report(y_test_car, y_pred_car))\n",
    "\n",
    "# Show feature importance for car type prediction\n",
    "car_feature_names = (\n",
    "    numerical_cols_car.tolist() +\n",
    "    car_type_pipeline.named_steps['preprocessor'].transformers_[1][1].get_feature_names_out(categorical_cols_car).tolist()\n",
    ")\n",
    "car_importances = car_type_pipeline.named_steps['classifier'].feature_importances_\n",
    "car_feature_importance = pd.DataFrame({\n",
    "    'Feature': car_feature_names,\n",
    "    'Importance': car_importances\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop factors influencing car type choice:\")\n",
    "print(car_feature_importance.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply car type recommendations to likely bookers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 recommendations:\n",
      "       User_ID Recommended_Car_Type  Recommendation_Confidence Alternative_1  \\\n",
      "21218      209            Hatchback                   0.930000        Luxury   \n",
      "15172      155               Luxury                   0.927500     Hatchback   \n",
      "21240      209               Luxury                   0.919167         Sedan   \n",
      "15264      155                Sedan                   0.902726        Luxury   \n",
      "21212      209               Luxury                   0.890000           SUV   \n",
      "15236      155               Luxury                   0.877167     Hatchback   \n",
      "21256      209            Hatchback                   0.818333         Sedan   \n",
      "15204      155            Hatchback                   0.817500           SUV   \n",
      "15240      155               Luxury                   0.810000     Hatchback   \n",
      "15152      155            Hatchback                   0.805000           SUV   \n",
      "\n",
      "       Alternative_1_Confidence Alternative_2  Alternative_2_Confidence  \\\n",
      "21218                  0.030000         Sedan                  0.022500   \n",
      "15172                  0.043333           SUV                  0.019167   \n",
      "21240                  0.051667           SUV                  0.029167   \n",
      "15264                  0.077274     Hatchback                  0.013333   \n",
      "21212                  0.062500         Sedan                  0.037500   \n",
      "15236                  0.060667           SUV                  0.038833   \n",
      "21256                  0.102139        Luxury                  0.064333   \n",
      "15204                  0.137250         Sedan                  0.045250   \n",
      "15240                  0.130000           SUV                  0.060000   \n",
      "15152                  0.111000         Sedan                  0.060667   \n",
      "\n",
      "       car_booking_likelihood  \n",
      "21218                0.976667  \n",
      "15172                0.976667  \n",
      "21240                0.976667  \n",
      "15264                0.976667  \n",
      "21212                0.976667  \n",
      "15236                0.976667  \n",
      "21256                0.976667  \n",
      "15204                0.976667  \n",
      "15240                0.976667  \n",
      "15152                0.976667  \n"
     ]
    }
   ],
   "source": [
    "# Get the data for users who are likely to book cars\n",
    "likely_bookers_data = recommendation_set_cleaned[recommendation_set_cleaned['User_ID'].isin(likely_car_bookers['User_ID'])]\n",
    "\n",
    "# Prepare features for car type prediction\n",
    "X_recommend_car_type = likely_bookers_data[car_type_features]\n",
    "\n",
    "# Predict car types\n",
    "recommended_car_types = car_type_pipeline.predict(X_recommend_car_type)\n",
    "car_type_probabilities = car_type_pipeline.predict_proba(X_recommend_car_type)\n",
    "\n",
    "# Get the top 3 recommended car types for each user\n",
    "n_classes = len(car_type_pipeline.classes_)\n",
    "top_3_indices = np.argsort(-car_type_probabilities, axis=1)[:, :3]\n",
    "top_3_car_types = car_type_pipeline.classes_[top_3_indices]\n",
    "top_3_probabilities = np.array([car_type_probabilities[i, top_3_indices[i]] for i in range(len(top_3_indices))])\n",
    "\n",
    "# Create final recommendations dataframe\n",
    "final_recommendations = pd.DataFrame({\n",
    "    'User_ID': likely_bookers_data['User_ID'].values,\n",
    "    'Recommended_Car_Type': recommended_car_types,\n",
    "    'Recommendation_Confidence': np.max(car_type_probabilities, axis=1),\n",
    "    'Alternative_1': top_3_car_types[:, 1],\n",
    "    'Alternative_1_Confidence': top_3_probabilities[:, 1],\n",
    "    'Alternative_2': top_3_car_types[:, 2],\n",
    "    'Alternative_2_Confidence': top_3_probabilities[:, 2]\n",
    "})\n",
    "\n",
    "# Merge with car booking likelihood\n",
    "final_recommendations = final_recommendations.merge(\n",
    "    likely_car_bookers[['User_ID', 'car_booking_likelihood']], \n",
    "    on='User_ID'\n",
    ")\n",
    "\n",
    "# Sort by booking likelihood and recommendation confidence\n",
    "final_recommendations = final_recommendations.sort_values(\n",
    "    by=['car_booking_likelihood', 'Recommendation_Confidence'], \n",
    "    ascending=False\n",
    ")\n",
    "\n",
    "print(\"\\nTop 10 recommendations:\")\n",
    "print(final_recommendations.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Generate insights and summaries for business use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distribution of primary car type recommendations:\n",
      "Recommended_Car_Type\n",
      "Hatchback    0.274817\n",
      "Sedan        0.265374\n",
      "SUV          0.231116\n",
      "Luxury       0.228693\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Average confidence by car type:\n",
      "Recommended_Car_Type\n",
      "Luxury       0.588657\n",
      "Hatchback    0.570219\n",
      "Sedan        0.564717\n",
      "SUV          0.559918\n",
      "Name: Recommendation_Confidence, dtype: float64\n",
      "\n",
      "Summary of recommendation campaign:\n",
      "Total users eligible for recommendations: 131211\n",
      "Average booking likelihood: 0.77\n",
      "Average recommendation confidence: 0.57\n",
      "\n",
      "Top 5 destinations for car recommendations:\n",
      "Arrival_place\n",
      "Rio de Janeiro (RJ)    630499\n",
      "Salvador (BH)          623044\n",
      "Natal (RN)             616395\n",
      "Sao Paulo (SP)         587741\n",
      "Recife (PE)            557090\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 1. Distribution of recommended car types\n",
    "print(\"\\nDistribution of primary car type recommendations:\")\n",
    "print(final_recommendations['Recommended_Car_Type'].value_counts(normalize=True))\n",
    "\n",
    "# 2. Average confidence by car type\n",
    "print(\"\\nAverage confidence by car type:\")\n",
    "confidence_by_type = final_recommendations.groupby('Recommended_Car_Type')['Recommendation_Confidence'].mean()\n",
    "print(confidence_by_type.sort_values(ascending=False))\n",
    "\n",
    "# 3. Summary statistics\n",
    "print(\"\\nSummary of recommendation campaign:\")\n",
    "print(f\"Total users eligible for recommendations: {len(final_recommendations)}\")\n",
    "print(f\"Average booking likelihood: {final_recommendations['car_booking_likelihood'].mean():.2f}\")\n",
    "print(f\"Average recommendation confidence: {final_recommendations['Recommendation_Confidence'].mean():.2f}\")\n",
    "\n",
    "# 4. Breakdown by Arrival_place\n",
    "place_breakdown = final_recommendations.merge(\n",
    "    likely_bookers_data[['User_ID', 'Arrival_place']], \n",
    "    on='User_ID'\n",
    ")\n",
    "top_places = place_breakdown.groupby('Arrival_place').size().sort_values(ascending=False).head(5)\n",
    "print(\"\\nTop 5 destinations for car recommendations:\")\n",
    "print(top_places)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improved Car Booking Likelihood Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical columns: ['Flight_price', 'Flight_duration', 'Flight_Distance', 'Hotel_stay', 'Hotel_per_day_price', 'Hotel_TotalPrice', 'distance_stay_ratio', 'total_trip_cost', 'flight_price_ratio']\n",
      "Categorical columns: ['Flight_agency', 'Departure_date', 'Arrival_place']\n",
      "\n",
      "Improved Model Performance:\n",
      "[[330 425]\n",
      " [271 484]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.44      0.49       755\n",
      "           1       0.53      0.64      0.58       755\n",
      "\n",
      "    accuracy                           0.54      1510\n",
      "   macro avg       0.54      0.54      0.53      1510\n",
      "weighted avg       0.54      0.54      0.53      1510\n",
      "\n",
      "ROC AUC Score: 0.5475\n",
      "\n",
      "Best parameters: {'classifier__learning_rate': 0.01, 'classifier__max_depth': 3, 'classifier__n_estimators': 100}\n",
      "Best ROC AUC: 0.2270\n",
      "\n",
      "Best Model Performance:\n",
      "[[104 651]\n",
      " [ 57 698]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.14      0.23       755\n",
      "           1       0.52      0.92      0.66       755\n",
      "\n",
      "    accuracy                           0.53      1510\n",
      "   macro avg       0.58      0.53      0.45      1510\n",
      "weighted avg       0.58      0.53      0.45      1510\n",
      "\n",
      "ROC AUC Score: 0.5317\n",
      "\n",
      "Number of users with car booking likelihood >= 0.7: 0\n",
      "Empty DataFrame\n",
      "Columns: [User_ID, car_booking_likelihood]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# 1. Feature Engineering - Modified to avoid the missing columns issue\n",
    "def engineer_features(df):\n",
    "    df_new = df.copy()\n",
    "    \n",
    "    # Create interaction features with existing columns only\n",
    "    if 'Flight_Distance' in df_new.columns and 'Hotel_stay' in df_new.columns:\n",
    "        df_new['distance_stay_ratio'] = df_new['Flight_Distance'] / (df_new['Hotel_stay'] + 1)  # +1 to avoid division by zero\n",
    "    \n",
    "    if 'Flight_price' in df_new.columns and 'Hotel_TotalPrice' in df_new.columns:\n",
    "        df_new['total_trip_cost'] = df_new['Flight_price'] + df_new['Hotel_TotalPrice']\n",
    "        df_new['flight_price_ratio'] = df_new['Flight_price'] / (df_new['total_trip_cost'] + 1)\n",
    "    \n",
    "    # Let's avoid date conversion since it's causing issues\n",
    "    # Instead, let's focus on the columns that exist in all datasets\n",
    "    \n",
    "    return df_new\n",
    "\n",
    "# Apply feature engineering to training data\n",
    "X_train_raw_enhanced = engineer_features(X_train_raw)\n",
    "\n",
    "# Identify column types after feature engineering\n",
    "numerical_cols = X_train_raw_enhanced.select_dtypes(include=['float64', 'int64']).columns\n",
    "categorical_cols = X_train_raw_enhanced.select_dtypes(include=['object']).columns\n",
    "print(f\"Numerical columns: {list(numerical_cols)}\")\n",
    "print(f\"Categorical columns: {list(categorical_cols)}\")\n",
    "\n",
    "# 2. Create improved pipeline with XGBoost\n",
    "improved_pipeline = Pipeline([\n",
    "    ('preprocessor', ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numerical_cols),\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "        ])),\n",
    "    ('classifier', XGBClassifier(\n",
    "        learning_rate=0.1,\n",
    "        n_estimators=200,\n",
    "        max_depth=5,\n",
    "        min_child_weight=1,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective='binary:logistic',\n",
    "        scale_pos_weight=1,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# 3. Train the improved model\n",
    "improved_pipeline.fit(X_train_raw_enhanced, y_train_raw)\n",
    "\n",
    "# 4. Apply feature engineering to test data AFTER identifying the columns\n",
    "X_test_raw_enhanced = engineer_features(X_test_raw)\n",
    "\n",
    "# 5. Evaluate on test set\n",
    "y_pred_improved = improved_pipeline.predict(X_test_raw_enhanced)\n",
    "y_pred_proba_improved = improved_pipeline.predict_proba(X_test_raw_enhanced)[:, 1]\n",
    "\n",
    "print(\"\\nImproved Model Performance:\")\n",
    "print(confusion_matrix(y_test, y_pred_improved))\n",
    "print(classification_report(y_test, y_pred_improved))\n",
    "print(f\"ROC AUC Score: {roc_auc_score(y_test, y_pred_proba_improved):.4f}\")\n",
    "\n",
    "# 6. Hyperparameter tuning (smaller grid to speed up the process)\n",
    "param_grid = {\n",
    "    'classifier__learning_rate': [0.01, 0.1],\n",
    "    'classifier__max_depth': [3, 5],\n",
    "    'classifier__n_estimators': [100, 200]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    improved_pipeline,\n",
    "    param_grid,\n",
    "    cv=3,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_raw_enhanced, y_train_raw)\n",
    "print(f\"\\nBest parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best ROC AUC: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Use the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_best = best_model.predict(X_test_raw_enhanced)\n",
    "y_pred_proba_best = best_model.predict_proba(X_test_raw_enhanced)[:, 1]\n",
    "\n",
    "print(\"\\nBest Model Performance:\")\n",
    "print(confusion_matrix(y_test, y_pred_best))\n",
    "print(classification_report(y_test, y_pred_best))\n",
    "print(f\"ROC AUC Score: {roc_auc_score(y_test, y_pred_proba_best):.4f}\")\n",
    "\n",
    "# 7. Apply feature engineering to recommendation set and get predictions\n",
    "X_recommendation_enhanced = engineer_features(X_recommendation)\n",
    "prediction_proba = best_model.predict_proba(X_recommendation_enhanced)[:, 1]\n",
    "\n",
    "# Create recommendations dataframe with improved model\n",
    "recommendations = pd.DataFrame({\n",
    "    'User_ID': user_ids,\n",
    "    'car_booking_likelihood': prediction_proba\n",
    "})\n",
    "\n",
    "# Sort by likelihood (highest first)\n",
    "recommendations = recommendations.sort_values('car_booking_likelihood', ascending=False)\n",
    "\n",
    "# Select users above a certain threshold for car recommendations\n",
    "threshold = 0.7\n",
    "likely_car_bookers = recommendations[recommendations['car_booking_likelihood'] >= threshold]\n",
    "\n",
    "print(f\"\\nNumber of users with car booking likelihood >= {threshold}: {len(likely_car_bookers)}\")\n",
    "print(likely_car_bookers.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (7546, 12)\n",
      "y_train shape: (7546,)\n",
      "X_test shape: (1510, 12)\n",
      "y_test shape: (1510,)\n",
      "\n",
      "Numerical columns: ['Flight_price', 'Flight_duration', 'Flight_Distance', 'Hotel_stay', 'Hotel_per_day_price', 'Hotel_TotalPrice', 'distance_stay_ratio', 'total_trip_cost', 'flight_price_ratio']\n",
      "Categorical columns: ['Flight_agency', 'Departure_date', 'Arrival_place']\n",
      "\n",
      "Improved Model Performance:\n",
      "\n",
      "Confusion Matrix:\n",
      "[[330 425]\n",
      " [271 484]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.44      0.49       755\n",
      "           1       0.53      0.64      0.58       755\n",
      "\n",
      "    accuracy                           0.54      1510\n",
      "   macro avg       0.54      0.54      0.53      1510\n",
      "weighted avg       0.54      0.54      0.53      1510\n",
      "\n",
      "\n",
      "Best parameters: {'classifier__learning_rate': 0.01, 'classifier__max_depth': 3, 'classifier__n_estimators': 100}\n",
      "Best accuracy: 0.2980\n",
      "\n",
      "Best Model Performance:\n",
      "\n",
      "Confusion Matrix:\n",
      "[[104 651]\n",
      " [ 57 698]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.14      0.23       755\n",
      "           1       0.52      0.92      0.66       755\n",
      "\n",
      "    accuracy                           0.53      1510\n",
      "   macro avg       0.58      0.53      0.45      1510\n",
      "weighted avg       0.58      0.53      0.45      1510\n",
      "\n",
      "\n",
      "Top recommendations:\n",
      "       User_ID  probability_class_0  probability_class_1  predicted_class  \\\n",
      "26214      870             0.410337             0.589663                1   \n",
      "8875       280             0.413714             0.586286                1   \n",
      "1768        60             0.414707             0.585293                1   \n",
      "15139      490             0.415505             0.584495                1   \n",
      "20228      655             0.415754             0.584246                1   \n",
      "\n",
      "       confidence  \n",
      "26214    0.589663  \n",
      "8875     0.586286  \n",
      "1768     0.585293  \n",
      "15139    0.584495  \n",
      "20228    0.584246  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# If y is one-hot encoded or has 2 columns, convert to single column\n",
    "if isinstance(y_train_raw, pd.DataFrame):\n",
    "    # If it's a DataFrame, convert to single column\n",
    "    y_train_1d = y_train_raw.iloc[:, 0]\n",
    "    y_test_1d = y_test.iloc[:, 0]\n",
    "elif isinstance(y_train_raw, np.ndarray):\n",
    "    # If it's a numpy array with 2 columns, take first column\n",
    "    y_train_1d = y_train_raw[:, 0]\n",
    "    y_test_1d = y_test[:, 0]\n",
    "else:\n",
    "    y_train_1d = y_train_raw\n",
    "    y_test_1d = y_test\n",
    "\n",
    "def engineer_features(df):\n",
    "    df_new = df.copy()\n",
    "    \n",
    "    if 'Flight_Distance' in df_new.columns and 'Hotel_stay' in df_new.columns:\n",
    "        df_new['distance_stay_ratio'] = df_new['Flight_Distance'] / (df_new['Hotel_stay'] + 1)\n",
    "    \n",
    "    if 'Flight_price' in df_new.columns and 'Hotel_TotalPrice' in df_new.columns:\n",
    "        df_new['total_trip_cost'] = df_new['Flight_price'] + df_new['Hotel_TotalPrice']\n",
    "        df_new['flight_price_ratio'] = df_new['Flight_price'] / (df_new['total_trip_cost'] + 1)\n",
    "    \n",
    "    return df_new\n",
    "\n",
    "# Apply feature engineering\n",
    "X_train_raw_enhanced = engineer_features(X_train_raw)\n",
    "X_test_raw_enhanced = engineer_features(X_test_raw)\n",
    "\n",
    "# Print shape information for debugging\n",
    "print(f\"X_train shape: {X_train_raw_enhanced.shape}\")\n",
    "print(f\"y_train shape: {y_train_1d.shape}\")\n",
    "print(f\"X_test shape: {X_test_raw_enhanced.shape}\")\n",
    "print(f\"y_test shape: {y_test_1d.shape}\")\n",
    "\n",
    "# Identify column types\n",
    "numerical_cols = X_train_raw_enhanced.select_dtypes(include=['float64', 'int64']).columns\n",
    "categorical_cols = X_train_raw_enhanced.select_dtypes(include=['object']).columns\n",
    "\n",
    "print(f\"\\nNumerical columns: {list(numerical_cols)}\")\n",
    "print(f\"Categorical columns: {list(categorical_cols)}\")\n",
    "\n",
    "# Create pipeline\n",
    "improved_pipeline = Pipeline([\n",
    "    ('preprocessor', ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numerical_cols),\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "        ])),\n",
    "    ('classifier', XGBClassifier(\n",
    "        learning_rate=0.1,\n",
    "        n_estimators=200,\n",
    "        max_depth=5,\n",
    "        min_child_weight=1,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective='binary:logistic',  # Changed to binary classification\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "improved_pipeline.fit(X_train_raw_enhanced, y_train_1d)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_improved = improved_pipeline.predict(X_test_raw_enhanced)\n",
    "y_pred_proba_improved = improved_pipeline.predict_proba(X_test_raw_enhanced)\n",
    "\n",
    "print(\"\\nImproved Model Performance:\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test_1d, y_pred_improved))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_1d, y_pred_improved))\n",
    "\n",
    "# Hyperparameter tuning\n",
    "param_grid = {\n",
    "    'classifier__learning_rate': [0.01, 0.1],\n",
    "    'classifier__max_depth': [3, 5],\n",
    "    'classifier__n_estimators': [100, 200]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    improved_pipeline,\n",
    "    param_grid,\n",
    "    cv=3,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_raw_enhanced, y_train_1d)\n",
    "print(f\"\\nBest parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best accuracy: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Use the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_best = best_model.predict(X_test_raw_enhanced)\n",
    "y_pred_proba_best = best_model.predict_proba(X_test_raw_enhanced)\n",
    "\n",
    "print(\"\\nBest Model Performance:\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test_1d, y_pred_best))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_1d, y_pred_best))\n",
    "\n",
    "# Make recommendations\n",
    "X_recommendation_enhanced = engineer_features(X_recommendation)\n",
    "prediction_proba = best_model.predict_proba(X_recommendation_enhanced)\n",
    "\n",
    "# Create recommendations dataframe\n",
    "recommendations = pd.DataFrame({\n",
    "    'User_ID': user_ids,\n",
    "    'probability_class_0': prediction_proba[:, 0],\n",
    "    'probability_class_1': prediction_proba[:, 1],\n",
    "    'predicted_class': best_model.predict(X_recommendation_enhanced)\n",
    "})\n",
    "\n",
    "# Add confidence score\n",
    "recommendations['confidence'] = recommendations[['probability_class_0', 'probability_class_1']].max(axis=1)\n",
    "\n",
    "# Sort by confidence\n",
    "recommendations = recommendations.sort_values('confidence', ascending=False)\n",
    "\n",
    "print(\"\\nTop recommendations:\")\n",
    "print(recommendations.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Improved Car Type Model Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Hatchback       0.22      0.23      0.23       188\n",
      "      Luxury       0.23      0.24      0.24       185\n",
      "         SUV       0.25      0.21      0.23       184\n",
      "       Sedan       0.27      0.28      0.27       198\n",
      "\n",
      "    accuracy                           0.24       755\n",
      "   macro avg       0.24      0.24      0.24       755\n",
      "weighted avg       0.24      0.24      0.24       755\n",
      "\n",
      "\n",
      "Final Recommendations (Top 10):\n",
      "Empty DataFrame\n",
      "Columns: [User_ID, Recommended_Car_Type, Recommendation_Confidence, Alternative_1, Alternative_1_Confidence, Alternative_2, Alternative_2_Confidence, car_booking_likelihood]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# 1. Feature Engineering for car type prediction (unchanged)\n",
    "def engineer_car_type_features(df):\n",
    "    df_new = df.copy()\n",
    "    \n",
    "    # Budget indicator features\n",
    "    if 'Flight_price' in df_new.columns and 'Hotel_per_day_price' in df_new.columns:\n",
    "        df_new['total_daily_budget'] = df_new['Flight_price'] / 7 + df_new['Hotel_per_day_price']\n",
    "    \n",
    "    # Trip length indicators\n",
    "    if 'Hotel_stay' in df_new.columns:\n",
    "        df_new['long_trip'] = (df_new['Hotel_stay'] > 7).astype(int)\n",
    "    \n",
    "    # Distance categories\n",
    "    if 'Flight_Distance' in df_new.columns:\n",
    "        df_new['distance_category_num'] = pd.cut(\n",
    "            df_new['Flight_Distance'],\n",
    "            bins=[0, 500, 1500, 3000, float('inf')],\n",
    "            labels=[1, 2, 3, 4]\n",
    "        ).astype(float)\n",
    "    \n",
    "    return df_new\n",
    "\n",
    "# 2. Add label encoding for target variable\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize and fit label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train_car)\n",
    "y_test_encoded = label_encoder.transform(y_test_car)\n",
    "\n",
    "# Apply feature engineering\n",
    "X_train_car_enhanced = engineer_car_type_features(X_train_car)\n",
    "X_test_car_enhanced = engineer_car_type_features(X_test_car)\n",
    "\n",
    "# Update columns after feature engineering\n",
    "numerical_cols_car = X_train_car_enhanced.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_cols_car = X_train_car_enhanced.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "# 3. Create improved car type pipeline\n",
    "car_type_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_cols_car),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols_car)\n",
    "    ])\n",
    "\n",
    "improved_car_type_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', car_type_preprocessor),\n",
    "    ('classifier', XGBClassifier(\n",
    "        learning_rate=0.1,\n",
    "        n_estimators=150,\n",
    "        max_depth=5,\n",
    "        objective='multi:softprob',\n",
    "        num_class=len(label_encoder.classes_),\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# 4. Train the improved car type model\n",
    "improved_car_type_pipeline.fit(X_train_car_enhanced, y_train_encoded)\n",
    "\n",
    "# 5. Evaluate on test set\n",
    "y_pred_encoded = improved_car_type_pipeline.predict(X_test_car_enhanced)\n",
    "y_pred_car_improved = label_encoder.inverse_transform(y_pred_encoded)\n",
    "print(\"\\nImproved Car Type Model Performance:\")\n",
    "print(classification_report(y_test_car, y_pred_car_improved))\n",
    "\n",
    "# 6. Apply improved car type model to likely bookers\n",
    "X_recommend_car_type_enhanced = engineer_car_type_features(X_recommend_car_type)\n",
    "recommended_car_types_encoded = improved_car_type_pipeline.predict(X_recommend_car_type_enhanced)\n",
    "car_type_probabilities = improved_car_type_pipeline.predict_proba(X_recommend_car_type_enhanced)\n",
    "\n",
    "# Get the top 3 recommended car types for each user\n",
    "top_3_indices = np.argsort(-car_type_probabilities, axis=1)[:, :3]\n",
    "\n",
    "# Create arrays for each alternative recommendation\n",
    "recommended_car_types = label_encoder.inverse_transform(recommended_car_types_encoded)\n",
    "alternative_1 = label_encoder.inverse_transform(top_3_indices[:, 1])\n",
    "alternative_2 = label_encoder.inverse_transform(top_3_indices[:, 2])\n",
    "\n",
    "# Get corresponding probabilities\n",
    "top_3_probabilities = np.array([car_type_probabilities[i, top_3_indices[i]] for i in range(len(top_3_indices))])\n",
    "\n",
    "# 7. Create final recommendations dataframe\n",
    "final_recommendations = pd.DataFrame({\n",
    "    'User_ID': likely_bookers_data['User_ID'].values,\n",
    "    'Recommended_Car_Type': recommended_car_types,\n",
    "    'Recommendation_Confidence': np.max(car_type_probabilities, axis=1),\n",
    "    'Alternative_1': alternative_1,\n",
    "    'Alternative_1_Confidence': top_3_probabilities[:, 1],\n",
    "    'Alternative_2': alternative_2,\n",
    "    'Alternative_2_Confidence': top_3_probabilities[:, 2]\n",
    "})\n",
    "\n",
    "# Merge with improved car booking likelihood\n",
    "final_recommendations = final_recommendations.merge(\n",
    "    likely_car_bookers[['User_ID', 'car_booking_likelihood']], \n",
    "    on='User_ID'\n",
    ")\n",
    "\n",
    "# Sort by booking likelihood and recommendation confidence\n",
    "final_recommendations = final_recommendations.sort_values(\n",
    "    by=['car_booking_likelihood', 'Recommendation_Confidence'], \n",
    "    ascending=False\n",
    ")\n",
    "\n",
    "print(\"\\nFinal Recommendations (Top 10):\")\n",
    "print(final_recommendations.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from datetime import datetime\n",
    "import lightgbm as lgb\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "class EnhancedCarRecommender:\n",
    "    def __init__(self):\n",
    "        self.booking_model = None\n",
    "        self.car_type_model = None\n",
    "        self.user_segments = None\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.version = \"1.0.0\"\n",
    "        self.recommendation_history = []\n",
    "        \n",
    "    def engineer_features(self, df):\n",
    "        \"\"\"Enhanced feature engineering with temporal and user segments\"\"\"\n",
    "        df_new = df.copy()\n",
    "        \n",
    "        # Basic features from original implementation\n",
    "        if 'Flight_Distance' in df_new.columns and 'Hotel_stay' in df_new.columns:\n",
    "            df_new['distance_stay_ratio'] = df_new['Flight_Distance'] / (df_new['Hotel_stay'] + 1)\n",
    "        \n",
    "        if 'Flight_price' in df_new.columns and 'Hotel_TotalPrice' in df_new.columns:\n",
    "            df_new['total_trip_cost'] = df_new['Flight_price'] + df_new['Hotel_TotalPrice']\n",
    "            df_new['flight_price_ratio'] = df_new['Flight_price'] / (df_new['total_trip_cost'] + 1)\n",
    "        \n",
    "        # Add temporal features\n",
    "        if 'Check_in_Hotel' in df_new.columns:\n",
    "            df_new['Check_in_Hotel'] = pd.to_datetime(df_new['Check_in_Hotel'])\n",
    "            df_new['month'] = df_new['Check_in_Hotel'].dt.month\n",
    "            df_new['day_of_week'] = df_new['Check_in_Hotel'].dt.dayofweek\n",
    "            df_new['is_weekend'] = df_new['day_of_week'].isin([5, 6]).astype(int)\n",
    "            df_new['is_holiday_season'] = df_new['month'].isin([7, 8, 12]).astype(int)\n",
    "        \n",
    "        # Add location-based features\n",
    "        if 'Arrival_place' in df_new.columns:\n",
    "            popular_destinations = ['New York', 'Los Angeles', 'Chicago', 'Miami', 'Las Vegas']\n",
    "            df_new['is_popular_destination'] = df_new['Arrival_place'].isin(popular_destinations).astype(int)\n",
    "        \n",
    "        return df_new\n",
    "    \n",
    "    def create_user_segments(self, df):\n",
    "        \"\"\"Create user segments based on behavior patterns\"\"\"\n",
    "        features_for_clustering = ['total_trip_cost', 'Hotel_stay', 'Flight_Distance']\n",
    "        \n",
    "        # Prepare data for clustering\n",
    "        cluster_data = df[features_for_clustering].copy()\n",
    "        scaler = StandardScaler()\n",
    "        cluster_data_scaled = scaler.fit_transform(cluster_data)\n",
    "        \n",
    "        # Perform clustering\n",
    "        kmeans = KMeans(n_clusters=4, random_state=42)\n",
    "        df['user_segment'] = kmeans.fit_predict(cluster_data_scaled)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def handle_missing_data(self, df):\n",
    "        \"\"\"Handle missing data with appropriate strategies\"\"\"\n",
    "        # Fill numerical columns with median\n",
    "        numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "        df[numerical_cols] = df[numerical_cols].fillna(df[numerical_cols].median())\n",
    "        \n",
    "        # Fill categorical columns with mode\n",
    "        categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "        df[categorical_cols] = df[categorical_cols].fillna(df[categorical_cols].mode().iloc[0])\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def prepare_temporal_cv(self, df):\n",
    "        \"\"\"Prepare time-based cross validation\"\"\"\n",
    "        tscv = TimeSeriesSplit(n_splits=5)\n",
    "        return tscv\n",
    "    \n",
    "    def fit(self, flights_data, hotels_data, cars_data):\n",
    "        \"\"\"Train the enhanced recommendation system\"\"\"\n",
    "        # Merge datasets\n",
    "        merged_data = self._merge_datasets(flights_data, hotels_data, cars_data)\n",
    "        \n",
    "        # Handle missing data\n",
    "        merged_data = self.handle_missing_data(merged_data)\n",
    "        \n",
    "        # Create user segments\n",
    "        merged_data = self.create_user_segments(merged_data)\n",
    "        \n",
    "        # Engineer features\n",
    "        X = self.engineer_features(merged_data)\n",
    "        \n",
    "        # Prepare target variables\n",
    "        y_booking = (merged_data['car_booking'] == 1).astype(int)\n",
    "        y_car_type = self.label_encoder.fit_transform(merged_data[merged_data['car_booking'] == 1]['carType'])\n",
    "        \n",
    "        # Train booking likelihood model\n",
    "        self.booking_model = self._train_booking_model(X, y_booking)\n",
    "        \n",
    "        # Train car type model\n",
    "        self.car_type_model = self._train_car_type_model(\n",
    "            X[merged_data['car_booking'] == 1],\n",
    "            y_car_type\n",
    "        )\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _train_booking_model(self, X, y):\n",
    "        \"\"\"Train an enhanced booking likelihood model\"\"\"\n",
    "        # Define features\n",
    "        numerical_cols = X.select_dtypes(include=['float64', 'int64']).columns\n",
    "        categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "        \n",
    "        # Create pipeline\n",
    "        pipeline = Pipeline([\n",
    "            ('preprocessor', ColumnTransformer([\n",
    "                ('num', StandardScaler(), numerical_cols),\n",
    "                ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "            ])),\n",
    "            ('classifier', lgb.LGBMClassifier(\n",
    "                objective='binary',\n",
    "                boosting_type='dart',\n",
    "                n_estimators=200,\n",
    "                learning_rate=0.1,\n",
    "                max_depth=5,\n",
    "                num_leaves=31,\n",
    "                feature_fraction=0.9\n",
    "            ))\n",
    "        ])\n",
    "        \n",
    "        # Train model\n",
    "        pipeline.fit(X, y)\n",
    "        return pipeline\n",
    "    \n",
    "    def _train_car_type_model(self, X, y):\n",
    "        \"\"\"Train an enhanced car type recommendation model\"\"\"\n",
    "        numerical_cols = X.select_dtypes(include=['float64', 'int64']).columns\n",
    "        categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "        \n",
    "        pipeline = Pipeline([\n",
    "            ('preprocessor', ColumnTransformer([\n",
    "                ('num', StandardScaler(), numerical_cols),\n",
    "                ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "            ])),\n",
    "            ('classifier', XGBClassifier(\n",
    "                objective='multi:softprob',\n",
    "                n_estimators=150,\n",
    "                max_depth=5,\n",
    "                learning_rate=0.1\n",
    "            ))\n",
    "        ])\n",
    "        \n",
    "        pipeline.fit(X, y)\n",
    "        return pipeline\n",
    "    \n",
    "    def recommend(self, user_data):\n",
    "        \"\"\"Generate recommendations with monitoring\"\"\"\n",
    "        # Engineer features\n",
    "        X = self.engineer_features(user_data)\n",
    "        X = self.handle_missing_data(X)\n",
    "        X = self.create_user_segments(X)\n",
    "        \n",
    "        # Get booking likelihood\n",
    "        booking_proba = self.booking_model.predict_proba(X)[:, 1]\n",
    "        \n",
    "        # Get car type recommendations for likely bookers\n",
    "        likely_bookers = booking_proba >= 0.7\n",
    "        car_type_proba = None\n",
    "        if likely_bookers.any():\n",
    "            car_type_proba = self.car_type_model.predict_proba(X[likely_bookers])\n",
    "        \n",
    "        # Create recommendations\n",
    "        recommendations = self._create_recommendation_df(\n",
    "            user_data,\n",
    "            booking_proba,\n",
    "            car_type_proba,\n",
    "            likely_bookers\n",
    "        )\n",
    "        \n",
    "        # Log recommendations\n",
    "        self._log_recommendations(recommendations)\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def _create_recommendation_df(self, user_data, booking_proba, car_type_proba, likely_bookers):\n",
    "        \"\"\"Create detailed recommendations dataframe\"\"\"\n",
    "        recommendations = pd.DataFrame({\n",
    "            'User_ID': user_data['User_ID'],\n",
    "            'booking_likelihood': booking_proba,\n",
    "            'timestamp': datetime.now(),\n",
    "            'model_version': self.version\n",
    "        })\n",
    "        \n",
    "        if car_type_proba is not None and likely_bookers.any():\n",
    "            # Get top 3 car types\n",
    "            top_3_indices = np.argsort(-car_type_proba, axis=1)[:, :3]\n",
    "            car_types = self.label_encoder.inverse_transform(range(car_type_proba.shape[1]))\n",
    "            \n",
    "            # Add car type recommendations\n",
    "            recommendations.loc[likely_bookers, 'recommended_car_type'] = car_types[top_3_indices[:, 0]]\n",
    "            recommendations.loc[likely_bookers, 'alternative_1'] = car_types[top_3_indices[:, 1]]\n",
    "            recommendations.loc[likely_bookers, 'alternative_2'] = car_types[top_3_indices[:, 2]]\n",
    "            \n",
    "            # Add confidence scores\n",
    "            recommendations.loc[likely_bookers, 'recommendation_confidence'] = np.max(car_type_proba, axis=1)\n",
    "            \n",
    "        return recommendations\n",
    "    \n",
    "    def _log_recommendations(self, recommendations):\n",
    "        \"\"\"Log recommendations for monitoring\"\"\"\n",
    "        self.recommendation_history.append({\n",
    "            'timestamp': datetime.now(),\n",
    "            'num_recommendations': len(recommendations),\n",
    "            'avg_booking_likelihood': recommendations['booking_likelihood'].mean(),\n",
    "            'model_version': self.version\n",
    "        })\n",
    "    \n",
    "    def get_performance_metrics(self):\n",
    "        \"\"\"Get system performance metrics\"\"\"\n",
    "        if not self.recommendation_history:\n",
    "            return \"No recommendations made yet\"\n",
    "        \n",
    "        metrics = pd.DataFrame(self.recommendation_history)\n",
    "        \n",
    "        return {\n",
    "            'total_recommendations': metrics['num_recommendations'].sum(),\n",
    "            'avg_booking_likelihood': metrics['avg_booking_likelihood'].mean(),\n",
    "            'recommendations_per_day': metrics.groupby(\n",
    "                metrics['timestamp'].dt.date)['num_recommendations'].mean()\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the recommender\n",
    "recommender = EnhancedCarRecommender()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "flights_data = pd.read_excel(r\"D:\\Make_my_trip\\FinalDataset\\FlightFINALdataset.xlsx\")\n",
    "hotels_data = pd.read_excel(r\"D:\\Make_my_trip\\FinalDataset\\HotelFINALdataset.xlsx\")\n",
    "cars_data = pd.read_excel(r\"D:\\Make_my_trip\\FinalDataset\\CarFINALdataset.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'EnhancedCarRecommender' object has no attribute '_merge_datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train the system\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mrecommender\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflights_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhotels_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcars_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 84\u001b[0m, in \u001b[0;36mEnhancedCarRecommender.fit\u001b[1;34m(self, flights_data, hotels_data, cars_data)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Train the enhanced recommendation system\"\"\"\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# Merge datasets\u001b[39;00m\n\u001b[1;32m---> 84\u001b[0m merged_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_datasets\u001b[49m(flights_data, hotels_data, cars_data)\n\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m# Handle missing data\u001b[39;00m\n\u001b[0;32m     87\u001b[0m merged_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_missing_data(merged_data)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'EnhancedCarRecommender' object has no attribute '_merge_datasets'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the system\n",
    "recommender.fit(flights_data, hotels_data, cars_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data = pd.read_excel(r\"D:\\Make_my_trip\\FinalDataset\\updated_user.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate recommendations\n",
    "recommendations = recommender.recommend(user_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get performance metrics\n",
    "metrics = recommender.get_performance_metrics()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recomm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
